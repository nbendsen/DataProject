---
title: "Model validation"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
    css: style.css
bibliography: references.bib
csl: brewingscience.csl
editor_options:
  chunk_output_type: console
---

---
title: "Model validation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(MASS)
library(survival)
library(tidyverse)
library(ggplot2)
```

<h2>Introduction</h2>

On the home page we introduced the beta binomial update method. The method combines observed cover data from the small square in the center of the plot and the presence/absence data from the entire plot. This gave rise to a new updated dataset that could be used to give diversity estimates of the entire plot. This page is intended check the goodness-of-fit of the proposed method against the observed cover data.

When we created the method we assumed that it would describe the data well. Especially, we assumed that the diversity estimates we would get by using the beta binomial update method
did not differ too much from the diversity estimates we would achieve by only using the observed cover data. However, this is only an assumption and not necessarily true. We did however assume that the diversity estimates would be a bit higher using the beta binomial update method instead of using the observed data, since we add a small value for species present in the plot, but not observed in the small square of the plot, which the cover data does not. One issue could be that in plots with low diversity, the prior distributions will change the data too much so that our model will estimate the diversity way too high compared to the diversity estimate we would get from the observed cover data of the small square. <br> 

The intention of this page is to validate whether this assumption is acceptable. It will be done by using the ideas of posterior predictive checks, which are presented below and further described in [@MimnoE3441] and [@Gelman].


<h2>Posterior predictive checks</h2>
In each plot we obtain a posterior distribution for each specie that have a $1$ in the corresponding presence/absence data of the plot. These posterior distributions are estimated using the observed cover data and observed presence/absence data.

For a given species in a plot the posterior distribution is given by 

$$
\text{Posterior} \sim Beta(a+y,b+n-y)
$$
where $a$ and $b$ are the parameters from the estimated prior distribution for the species, $y$ is the number of hits by the species in the cover data and $n$ is the total number of possible hits for a species in a plot. In the case of the NOVANA dataset, we have $n = 16$.


In our method we use the mean of the posterior distribution as an estimate of the species cover in the entire plot. Instead of using the mean for the validation we draw one sample from the posterior distribution for each species we know is present in the plot and use this as the estimate of the species cover in the entire the plot. We will refer to this as generated cover data. After the sampling we apply some test statistic of interest on the generated cover data for the plot. This test statistic should capture some of the aspects on the data we interested in. In our case we want the method to be used to estimate diversity, so using some sort of diversity estimator as test statistic is natural. We do this for all plots in the dataset.

We repeat the above process a 1000 times so that we get a distribution of generated test statistics for each plot. Lastly, we compare the test statistic we would get by using the observed cover data with the distributions of generated test statistics from the generated data. Again we do this for each plot. We refer to the test statistic we get from the observed cover data as the observed test statistic. 


The idea behind posterior predictive checks is as follow: If the model assumption are appropriate, the generated data will look like the observed data viewed through the chosen test statistic. Escpecially, if we make a histogram of the generated test statistics the observed test statistic should not be an outlier. If it turns out that the observed test statistic is extreme compared to the generated test statistics it would cause some concern regarding whether the model is appropriate.

The posterior predictive check can be done visually as is done for the first 3 plots in the examples below.
In addition to visual inspections of the histograms we can also calculate the tail-area probability, which we call the posterior predictive p-value [3]. If we let $T(y)$ be the observed test statistic and $T(y^{rep})$ be the distribution of generated test statistics, then we calculate the posterior predictive p-value as

$$
\text{posterior predictive p-value} = 2\cdot \min\Big(P(T(y^{rep}) \leq T(y)),P(T(y^{rep}) \geq T(y)) \Big)
$$

A small posterior predictive p-value close to zero indicates that the observed test statistic is not very likely relative to the generated test statistics and the posterior predictive check suggests that the model is misspecified with respect to the test statistic. The reason is, that when generating the test statistics and and forming the histogram, we observe a distribution for the generated test statistics. When the posterior predictive p-value is small, the observed test statistic will have a small probability of occurring in the generated distribution for the test statistics, hence will not be likely to occur and thereby is not similar to the generated test statistics.
<br>
As mentioned earlier we will use diversity estimates as test statistics. We saw on the home page that most plots in the beta binomial cover dataset had higher diversity estimates than in the observed dataset. However, that was not the case for all plots. We have defined the posterior predictive p-value as above to take into account when our method returns diversity estimate too high or too low relative to the observed dataset as we see both outcomes as extreme. 

The posterior predictive checks are only intended to highlight if our model is likely given the data, i.e. the model fit the observed data well. This, however, does not mean that the model is "better" to estimate diversity than just using cover or presence/absence data. This is a further assumption we need to make based on considerations within the domain of ecology. 

<h2> Model validation for 1. example </h2>
In the first example on the home page we worked with a subset of the NOVANA dataset from the tertiary habitat "Surt overdrev". For further details on how this subset was created click [link](data_6230.html). In this subsection we will apply the ideas of posterior predictive checks to see how the beta binomial update method performs on this subset.
<br>

As test statistic we will use the Hill Simpson and Hill Shannon diversities that were also used in the example on the home page. Especially, we saw in the example on the home page how the diversity estimates obtained by the beta binomial cover dataset drifted further apart from the diversity estimates of the observed cover data when applying the the Hill Shannon diversity instead of the Hill Simpson diversity. Therefore, it is of interest to see how the properties of the two different ways to estimate diversity unfold when they are used as test statistics in our posterior predictive checks. Once again we define these as 
$$
\text{Hill Simpson = } \frac{1}{\Sigma_{i=1}^S(p_i)^2}
$$
$$
\text{Hill Shannon  = } e^{ - \Sigma_{i = 1}^Sp_i\cdot \ln(p_i)}
$$
Further details on the diversity estimates can be found on [link](Diversity.html)


We read in the datasets and remove the first 3 columns, since they do not contain information on species
```{r}
#Here we load the datasets for habitat 6230 in year 2014
cover <- read.csv("data/cover_data_6230_year2014.csv")
freq <- read.csv("data/frekvens_data_6230_year2014.csv")
#We remove the first 3 columns as they are not species
cover_data <- cover[,4:ncol(cover)]
freq_data <- freq[,4:ncol(freq)]
```
With the data we make a visual inspection of the first 3 plots where the Hill Shannon diversity is used as a test statistic. A description of the code implementation of the ppc()-function can be found in the section "Code implementation" [link](model_val.R).
```{r, include= FALSE}
#We load the function for used for the validation
source("code/model_val.R")
source("code/function.R")
```

```{r, warning=FALSE}
ppc(1, freq_data, cover_data)
```

```{r, warning=FALSE}
ppc(2, freq_data, cover_data)
```

```{r, warning=FALSE}
ppc(3, freq_data, cover_data)
```

All posterior predictive p-values are big in the above histograms which suggests that the model is appropriate for the first three plots viewed through the Hill Shannon diversity. If we run the posterior predictive check on all plots we can get the proportion of posterior predictive p-values that are less than 0.05. We use a posterior predictive p-value of 0.05 as a threshold for when to say that the observed test statistic is not very likely given the model.

```{r}
p_values_hill_shannon <- read.csv("data/hill_shannon_pval2.csv")
sum(p_values_hill_shannon < 0.05) / nrow(cover)
```

The interpretation is that for around 8% of the plots the posterior predictive check suggests that the beta binomial update method is not appropriate, i.e. the observed Hill Shannon diversity is an outlier compared to the Hill Shannon diversities that the model would generate.

However, it is important to check if these plots are randomly placed or lumped together for either high or low diversities. To do this we plot the mean of all the generated Hill Shannon diversities for each plot against the observed Hill Shannon diversity for the corresponding plot. We color each point in the scatter plot to visualize the posterior predictive p-value of the plot
```{r, include = FALSE}
generated_hill_shannon <- read.csv("data/hill_shannon.csv")[,1:1000]
observed_hill_shannon <- hill_shannon(cover_data)
colnames(p_values_hill_shannon) <- "p_values"
p_values_hill_shannon$mean <- rowMeans(generated_hill_shannon)
p_values_hill_shannon$observed <- observed_hill_shannon

b <- max(p_values_hill_shannon$p_values)
p_values_hill_shannon$p_values <- ifelse(p_values_hill_shannon$p_values==b, 1, p_values_hill_shannon$p_values)
```


```{r, echo = FALSE}
ggplot(data = p_values_hill_shannon, mapping = aes(x = observed, y = mean)) +
  geom_point(aes(col = p_values))+
  ylab("Mean of generated Hill Shannon diversity")+
  xlab("Hill Shannon diversity of observed cover data") +
  ggtitle("Mean of generated Hill Shannon diversity vs observed Hill Shannon diversity") +
  geom_abline()+
  scale_color_gradient(low="blue", high="red")+
  labs(color = "Posterior predictive p-value")
```
It is worth to notice that it is specially plots with low diversity that our model seems to give diversity estimates that differ a lot from what we would get from the observed cover data. However, this does not necessarily mean that our model is bad. What it does mean is that it is more likely for these plots that the beta binomial update method overestimate the diversity for the entire plot relative to the observed cover data of the small square. Otherwise the beta binomial update method seems to generate data that is aligned with the observed cover data.

Lastly, we will create a similar plot but with the Hill Simpson diversity used as test statistic. In the example on the home page we saw that the Hill Simpson diversity was the diversity estimator that gave a diversity estimate that differed the least between the beta binomial updated cover data and the observed cover data. 


```{r, include = FALSE}
generated_hill_simpson <- read.csv("data/hill_simpson.csv")[,1:1000]
p_values_hill_simpson <- read.csv("data/hill_simpson_pval2.csv")
observed_hill_simpson <- hill_simpson(cover_data)
colnames(p_values_hill_simpson) <- "p_values"
p_values_hill_simpson$mean <- rowMeans(generated_hill_simpson)
p_values_hill_simpson$observed <- observed_hill_simpson

a <- max(p_values_hill_simpson$p_values)
p_values_hill_simpson$p_values <- ifelse(p_values_hill_simpson$p_values==a, 1, p_values_hill_simpson$p_values)
```


```{r, echo = FALSE}
ggplot(data = p_values_hill_simpson, mapping = aes(x = observed, y = mean )) +
  geom_point(mapping = aes(col = p_values))+
  ylab("Mean of generated Hill Simpson diversity")+
  xlab("Hill Simpson diversity of observed cover data") +
  geom_abline()+
  scale_color_gradient(low="blue", high="red")+
  labs(color = "Posterior predictive p-value")
```

Again we see that it is only for plots with low diversity where the observed Hill Simpson diversity seems unlikely given the model. However, this is only a very small proportion.

```{r}
sum(p_values_hill_simpson$p_values < 0.05)/nrow(cover)
```
This shows that when the Hill Simpson diversity is used as test statistic the model will generate data that looks very much like the observed data. Much more than the case was when we used the Hill Shannon diversity as test statistic.

This matches with what we found in the "Hill Diversity comparison" section on the home page. There we also saw that the beta binomial update method has less importance viewed through the Hill Simpson diversity than through the Hill Shannon diversity.

For both test statistics the conclusion is that the model mostly generated data that looks like the observed data. This gives confidence in the fact that the model has captured some good aspects of the observed data and gives sensible and thereby useful result to work with. Again it should be emphasized that it does not mean the the model gives better diversity estimates then what could be obtained by only using the observed cover data.


<h2>Model validation for 2. example </h2>
We also want to include the posterior predictive checks we get when we apply these to the data from tertiary habitat "Kalkoverdrev" from year 2009 that we used in our second example on the home page. We make similar plots as we did above. First with the Hill Shannon diversity as test statistic and then with the Hill Simpson diveristy as test statistic.



```{r, include = FALSE}
cover_6210 <- read.csv("data/cover_data_6210_year2009.csv")
generated_hill_shannon6210 <- read.csv("data/hill_shannon_6210.csv")[,1:1000]
observed_hill_shannon6210 <- hill_shannon(cover_6210, 3)
p_values <- read.csv("data/6210_hill_shannon_pvalues2.csv")
colnames(p_values) <- "p_values"
p_values$mean <- rowMeans(generated_hill_shannon6210)
p_values$observed <- observed_hill_shannon6210
p_values$p_values <- ifelse(p_values$p_values>1, 1,p_values$p_values)
```




```{r, include = FALSE}
cover_6210 <- read.csv("data/cover_data_6210_year2009.csv")
generated_hill_simpson6210 <- read.csv("data/hill_simpson_6210.csv")[,1:1000]
observed_hill_simpson6210 <- hill_simpson(cover_6210, 3)
p_values_simpson <- read.csv("data/6210_hill_simpson_pvalues2.csv")
colnames(p_values_simpson) <- "p_values"
p_values_simpson$mean <- rowMeans(generated_hill_simpson6210)
p_values_simpson$observed <- observed_hill_simpson6210
```



```{r, echo = FALSE}
ggplot(data = p_values, mapping = aes(x = observed, y = mean)) +
  geom_point(aes(col = p_values))+
  ylab("Mean of generated Hill Shannon diversity")+
  xlab("Hill Shannon diversity of observed cover data") +
  ggtitle("Mean of generated Hill Shannon diversity vs observed Hill Shannon diversity") +
  geom_abline() +
  scale_color_gradient(low="blue", high="red")+
  labs(color = "Posterior predictive p-value")
```

```{r, echo = FALSE}
ggplot(data = p_values_simpson, mapping = aes(x = observed, y = mean )) +
  geom_point(mapping = aes(col = p_values ))+
  ylab("Mean of generated Hill Simpson diversity")+
  xlab("Hill Simpson diversity of observed cover data") +
  ggtitle("Mean of generated Hill Simpson diversity vs observed Hill Simpson diversity") +
  geom_abline()+
  scale_color_gradient(low="blue", high="red", guide = "colourbar")+
  labs(color = "Posterior predictive p-value")
```

Again we are interested in the proportion of plots where the observed test statistic is an outlier compared to the distribution of generated test statistics. For the Hill Shannon diversity the proportion is almost 13% if we again use a posterior predictive p-value of 0.05 as a threshold
```{r}
sum(p_values$p_values < 0.05)/nrow(cover_6210)
```

And the proportion of plots where the observed Hill Simpson diversity is an outlier relative to the generated Hill Simpson diversities is only around 2%
```{r}
sum(p_values_simpson$p_values < 0.05)/nrow(cover_6210)
```

In the case of both test statistics we see that a larger proportion of the plots have posterior predictive p-values indicating that the observed test statistic is unlikely under the model than in "surt overdrev" (first example). As an example, in "surt overdrev" the proportion of posterior predictive p-values below 0.05 with the Hill Shannon used as test statistic was only around 8%. However, the proportion of plots is still not so big that it causes concerns regarding the whether the assumptions underlying the model are appropriate. 

Below we study if there are some differences in the data from "surt overdrev" and "kalkoverdrev" that could explain why the model perform worse for the "kalkoverdrev" in these posterior predictive checks. 

<h2> Comparison of tertiary habitat types </h2>

```{r, include= FALSE}
freq_6210 <- read.csv("data/frekvens_data_6210_year2009.csv")
```

```{r, include = FALSE}
stats_6230 <- cover[1:3]
stats_6210 <- cover_6210[1:3]
stats_6230$freq_sum_6230 <- rowSums(freq[4:ncol(freq)])
stats_6210$freq_sum_6210 <- rowSums(freq_6210[4:ncol(freq_6210)])
mean_freq_6230 <- mean(stats_6230$freq_sum_6230)
mean_freq_6210 <- mean(stats_6210$freq_sum_6210)
stats_6230$cover_sum_6230 <- rowSums(cover[4:ncol(cover)] >  0)
stats_6210$cover_sum_6210 <- rowSums(cover_6210[4:ncol(cover_6210)] > 0 )
stats_6230$cover_mean_over_0_6230 <- rowMeans(replace(cover[4:ncol(cover)], cover[4:ncol(cover)] == 0, NA), na.rm = TRUE)
stats_6210$cover_mean_over_0_6210<- rowMeans(replace(cover_6210[4:ncol(cover_6210)], cover_6210[4:ncol(cover_6210)] == 0, NA), na.rm = TRUE)
mean_cover_mean_over_0_6230 <- mean(stats_6230$cover_mean_over_0_6230)
mean_cover_mean_over_0_6210 <- mean(stats_6210$cover_mean_over_0_6210)
mean_cover_6230 <- mean(stats_6230$cover_sum_6230)
mean_cover_6210 <- mean(stats_6210$cover_sum_6210)
stats_6230$difference_6230 <- stats_6230$freq_sum_6230 - stats_6230$cover_sum_6230
stats_6210$difference_6210 <- stats_6210$freq_sum_6210 - stats_6210$cover_sum_6210
stats_6230$proportion_6230 <- stats_6230$cover_sum_6230 / stats_6230$freq_sum_6230 
stats_6210$proportion_6210 <- stats_6210$cover_sum_6210 / stats_6210$freq_sum_6210
mean_proportion_6230 <- mean(stats_6230$proportion_6230)
mean_proportion_6210 <- mean(stats_6210$proportion_6210)
stats_6210$pval <- p_values$p_values
stats_6230$pval <- p_values_hill_shannon$p_values
```

```{r, include = FALSE}
mean(p_values_hill_shannon$observed[p_values_hill_shannon$p_values < 0.05])
mean(p_values$observed[p_values$p_values < 0.05])
```

```{r, include = FALSE}
mean(p_values_hill_shannon$observed)
mean(p_values$observed)
```

```{r, include=FALSE, eval=FALSE}
#For simplicity we will only be using the Hill Shannon to estimate #diversity in the following. Applying the Hill Shannon diversity to the #observed cover data for "kalkoverdrev" we get that the average diversity #in the plots is 7.5 while the average Hill Shannon diversity for the #plots in "surt overdrev" is 5.9. This is a rather big difference and one #would might suspect that the beta binomial update method would have had #an bigger impact for more plots in "surt overdrev" than in "kalkoverdrev" #because it on average has less diversity.
```

First of all, we look at some statistics to compare the plots in "kalk overdrev" (6210 in NOVANA) and the plots in "surt overdrev" (6230 in NOVANA). If we compare the two tertiary habitat types we see that the number of species in the habitat type "kalk overdrev" is generally higher than in the habitat type "surt overdrev"  at plot level. 
<br>
We look at the presence/absence data for the "surt overdrev" and "kalk overdrev" respectively. For each dataset we sum the number of present species in each plot and take the mean of all these. In the presence/absence data for "kalk overdrev" the mean number of species in a plot is 32.6 species while it is 26.8 species in the presence/absence data for "surt overdrev".
<br>

We do the same with the cover data for both datasets respectively, i.e. we sum the number of species in a plot if the species is hit by at least one pin and take the mean over all plots. In the observed cover data for "kalkoverdrev" the average observed number of species in a plot is 10.5 while it is 8.3. in the observed cover data for "surt overdrev".
<br>

What is really interesting is the the proportion of a plot where a species has a $1$ in the presence/absence data and at least one observation in the corresponding cover data. If this proportion is low  we suspect these plots to be the ones where the beta binomial update method has the biggest impact. For each plot we take the number of species found in the cover data and divide this by the number of species found in the presence/absence data for that plot. A species is found in the cover data for a plot if it is hit be at least one pin. If we again just take the average we find that the average proportion for each plot in "kalk overdrev" is 0.33 and the average proportion for each plot in "surt overdrev" is 0.32. Again there are no big differences in the statistics between the two tertiary habitat types. 
<br><br>
However, we want to study this last statistic a bit further. To visualize the distribution of the proportions from the plots in each of the two tertiary habitat types we make the following histograms

```{r, include=FALSE}
prop <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(prop) <- c("Tertiary_habitat", "proportion")
for (ele in stats_6210$proportion_6210){
  prop[nrow(prop)+1,] <- c("Kalkoverdrev", ele)
  
}
for (ele in stats_6230$proportion_6230){
  prop[nrow(prop)+1,] <- c("Surt overdrev", ele)
  
}

prop$proportion <- as.numeric(prop$proportion)
```


```{r, echo = FALSE, message=FALSE, warning = FALSE}
ggplot()+
  geom_histogram(data=prop, aes(x = proportion, fill=Tertiary_habitat))+
  facet_wrap(~Tertiary_habitat)+
  theme_minimal()+
  scale_fill_manual(values=c("dodgerblue3", "sienna3"))+
  labs(fill = "Tertiary habitat", alpha = 0.9)+
  xlab("Proportion of species in presence/absence data also found in cover")
```

There are no big differences between the two histograms. The most important thing to notice from this histogram is that the histogram for "kalk overdrev" has more plots with a very low proportion. In "kalkoverdrev" 1.9% of the plots have proportion below 0.1 while this is only the case for 1.2% of the plots in "surt overdrev".
<br>

We suspect that the beta binomial update method has most impact on these plots with low proportion. To illustrate this point we plot posterior predictive p-value for each plot against the proportion of a plot where species are found in both cover data and presence/absence data. The posterior predictive p-values come from the posterior predictive checks where the Hill Shannon was used as test statistic. The y-axis is -log(posterior predictive p-value) and the dashed line is equal to -log(0.05) for comparison. Because we have taken the negative logarithm on the posterior predictive p-value the plots above the dashed line are more extreme.
```{r,include=FALSE}
stats_6230$pval1 <- -log(stats_6230$pval)
stats_6210$pval1 <- -log(stats_6210$pval)

```

```{r, include=FALSE}
stat <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(stat) <- c("Tertiary_habitat", "proportion", "pvalue")
for (ele in 1:nrow(stats_6210)){
  stat[nrow(stat)+1,] <- c("Kalkoverdrev", stats_6210[ele, 8], stats_6210[ele, 10])
  
}
for (ele in 1:nrow(stats_6230)){
  stat[nrow(stat)+1,] <- c("Surt overdrev", stats_6230[ele, 8], stats_6230[ele, 10])
}
```

```{r, include = FALSE}
stat$proportion <- as.numeric(stat$proportion)
stat$pvalue <- as.numeric(stat$pvalue)
```



```{r, echo = FALSE, message=FALSE, warning = FALSE}
ggplot() +
  geom_point(data = stat, aes(x= proportion, y=pvalue, col = Tertiary_habitat))+
  geom_smooth(data = stat, aes(x= proportion, y=pvalue))+
  facet_wrap(~Tertiary_habitat)+
  scale_y_continuous("-log(posterior predictive p-values)")+
  geom_hline(yintercept=-log(0.05), color = "black", linetype="dashed", alpha = 0.5) +
  scale_colour_discrete("Tertiary habitat")+
  scale_x_continuous("Proportion of species in presence/absence data also found in cover")+
  theme_minimal()
 
```


The above scatterplots nicely visualize the fact that plots with a low proportion are more sensitive to the beta binomial update method. We see that when a plot has a low proportion the observed Hill Shannon diversity is far more likely to be different than the generated Hill Shannon diversities we get from the beta binomial update method. Furthermore, we notice that all plots with a posterior predictive p-value less than 0.05 have a proportion less than 0.34. This fits nicely with what we expected and is probably the reason that our method has a slightly bigger effect on "kalk overdrev" than "surt overdrev" as "kalk overdrev" has a bit more plots where the proportion of species that are found in the presence/absence data are also found in the cover data is very low.

<h2>Other validations </h2>
By using the posterior predictive checks we studied if the observed data is a likely outcome of our constructed model. It turned out to be the conclusion in most cases. However, the observed data is only a sample for each plot. This means that the "true" diversity of a plot might be rather different than the information we get from the observed cover and presence/absence data. The more correct diversity estimate could be obtained if we had cover data for the entire plot and not only the small square in the center of the plot, this will however be a huge job to collect. The beta binomial update method that we have created should in theory help to give a better diversity estimate based on the sample from the cover data in the small square and the corresponding presence/absence data of the entire plot, since we include all species present in the entire plot.

An interesting thing to do would be to simulate a dataset (both cover and present/absence) with known diversities for the plots in the dataset. When knowing the true diversity we could have tested how much better our method would be to estimate the diversity rather than only using the cover or presence/absence data. However, this validation of the method has not been possible within the scope of this project and is a topic for further studies in the future.
<br><br>


<h2>Code implementation </h2>
The following is an implementation of the ppc() function that was used in the make visual inspections of the posterior predictive checks for some plots.
```{r, eval=FALSE}
library(fitdistrplus)
```
```{r, eval = FALSE}
#We read the cover data and the presence/absense data without the first 4 columns, as they do not cotains information on species
cover_data <- cover[,4:ncol(cover)]
freq_data <- freq[,4:ncol(freq)]
  
#We make a dataframa for the parameters of the prior distribution for each plot, so it is possible to save the parameteres, and not calculate them when making the posteriror for each plot. Each row will contain the number/name of the specie and its corresponding parameters for the prior 
beta_fit <- data.frame(matrix(ncol = 3, nrow = 0))
# We name the columns in the 
colnames(beta_fit) <- c("species","a", "b")
```
```{r, eval = FALSE}
#Here we calculate the parameters for the priror distributions of each specie:
for (specie in colnames(cover_data)) {
  #First we normalise. Since there is in total used 16 pins for each plot, we will devide the entries in the cover data by 16
  beta_data <- cover_data[,specie]/16
  
  #Now we remove the plots where the specie is not present. This can be done by using the information from the presence, absense data. If it        contains a 1, then the specie is present in the plot, if 0 it is absent. 
  beta_data <- beta_data[freq_data[[specie]] == 1]
    
  #If the specie is not present in any of the plots, we do not have information to make a prior distribution for it, and will just give it parameters a=0 and b=0 as seen in the else clause.
  if (length(unique(beta_data)) > 1) {
    #We use the method of moments to fit the prior beta distribution
    beta_data_fitted <- fitdist(beta_data, "beta", method = "mme")
    
    #The parameters are added to the dataframe
    beta_fit[nrow(beta_fit) + 1,] <- c(specie, beta_data_fitted$estimate[1], beta_data_fitted$estimate[2])
      
  }
  else {
    beta_fit[nrow(beta_fit) + 1,] <- c(specie, 0,0)
    
  }
}
```
```{r, eval = FALSE}
#n is the row number of the plot we are working with
n <- 1
#We define which species are in present in the plot
species_spotted_in_frekvens <- colnames(freq_data[c(freq_data[n,]  == 1)])
#We define which species are present in the present/absent dataset but are not seen in the cover dataset
not_in_cover <- setdiff(species_spotted_in_frekvens,colnames(cover_data))
#We remove the species, that at present in the plots in the present/absent data, but are not observed in any plots in the cover data
species_spotted_in_frekvens <- setdiff(species_spotted_in_frekvens, not_in_cover)
# we remove the columns, that are not representing species
observed <- cover_data[n,c(species_spotted_in_frekvens)]
tmp <- observed[observed > 0]
T_static <- -sum(tmp/sum(observed) * log((tmp/sum(observed))))
#We make a dataframe to save the parameters of the posterior for each spotted specie in the plot
new_beta <- data.frame(matrix(ncol = 3, nrow = 0))
  
colnames(new_beta) <- c("species","a", "b") 
for (species_spotted in species_spotted_in_frekvens ) {
      
  #We define the parameters for the posterior 
      alpha_post <- as.numeric(beta_fit[beta_fit$species == species_spotted,]$a) + cover_data[[species_spotted]][n] 
    beta_post <-  as.numeric(beta_fit[beta_fit$species == species_spotted,]$b) + 16 - cover_data[[species_spotted]][n]
    
      #If the parameters are to small, we change them to 0, since R has a hard time working with them
      alpha_post <- ifelse(alpha_post < 1e-10, 0, alpha_post)
      beta_post <- ifelse(beta_post < 1e-10,0, beta_post)
      
      #We add the parameters to the dataframe
      new_beta[nrow(new_beta) + 1,] <- c(species_spotted, alpha_post, beta_post)}
#We make a vector, to save the shannon indexes produces in each iteration
shannon <- c()
for (i in 1:1000){
  
  #Vector for saving the random generated values from the posterior of each specie
  values <- c()
  for (ele in species_spotted_in_frekvens){
    #These are the parameters, for the posterior of the specie
    a <- as.numeric(new_beta[new_beta$species == ele,]$a)
    b <- as.numeric(new_beta[new_beta$species ==ele,]$b)
    
    # We draw a random number from a beta distribution with the parameters for that specie and add it to the vector
    values <- c(values, rbeta(1,a,b))
    
    
  }
  #We remove the values that are to small
  tmp <- values[ values > 0.00001]
  total <- sum(tmp)
  
  #We calculate the Hill shannon diversity
  shannon <- c(shannon,exp(-sum(tmp/total * log((tmp/total)))))
  
}
min_val <- min(T_static, min(shannon)) - 0.1
max_val <- max(T_static, max(shannon)) + 0.1
n <- length(shannon)
pvalue <- 2*min(sum(shannon>= T_static)/n, sum(shannon<= T_static)/n)
#This is the code that produces the histogram over the generated data
hist(shannon, xlim = c(min_val, max_val), main = sprintf("Histogram of simulated Hill Shannon diversities for plot %d", n), xlab = "Hill Shannon diversity")
legend("topright", legend = "Red line is observed \n Hill Shannon index")
abline(v = T_static, col = "red" )
```
<h2>References </h2>


