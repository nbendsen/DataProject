<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Model validation</title>

<script src="site_libs/header-attrs-2.5/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/master/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">DataProject</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Introduction.html">Introduction</a>
</li>
<li>
  <a href="Diversity.html">Diversity</a>
</li>
<li>
  <a href="Model.html">Bayesian model</a>
</li>
<li>
  <a href="Function_usage.html">Functions</a>
</li>
<li>
  <a href="example.html">Examples</a>
</li>
<li>
  <a href="model_validering.html">Model validation</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/nbendsen/DataProject">
    <span class="fa fa-github"></span>
     
    Source code
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Model validation</h1>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2021-05-16
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 6 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 1
</p>
<p>
<strong>Knit directory:</strong> <code>DataProject-14-05/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.6.2). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguncommittedchanges"> <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> <strong>R Markdown file:</strong> uncommitted changes </a>
</p>
</div>
<div id="strongRMarkdownfilestronguncommittedchanges" class="panel-collapse collapse">
<div class="panel-body">
<p>The R Markdown file has unstaged changes. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run <code>wflow_publish</code> to commit the R Markdown file and build the HTML.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20210322code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20210322)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20210322code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20210322)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomnbendsenDataProjecttree7b30631fa9c179f345edec9da13fd5e338ec4466targetblank7b30631a"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/nbendsen/DataProject/tree/7b30631fa9c179f345edec9da13fd5e338ec4466" target="_blank">7b30631</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomnbendsenDataProjecttree7b30631fa9c179f345edec9da13fd5e338ec4466targetblank7b30631a" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.
</p>
<p>
The results in this page were generated with repository version <a href="https://github.com/nbendsen/DataProject/tree/7b30631fa9c179f345edec9da13fd5e338ec4466" target="_blank">7b30631</a>. See the <em>Past versions</em> tab to see a history of the changes made to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rproj.user/

Untracked files:
    Untracked:  analysis/Introduction.Rmd
    Untracked:  analysis/Model.Rmd
    Untracked:  analysis/example.Rmd
    Untracked:  analysis/style.css

Unstaged changes:
    Modified:   analysis/Diversity.Rmd
    Modified:   analysis/Function_usage.Rmd
    Modified:   analysis/_site.yml
    Modified:   analysis/index.Rmd
    Modified:   analysis/model_validering.rmd
    Modified:   analysis/plot3.PNG

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were made to the R Markdown (<code>analysis/model_validering.rmd</code>) and HTML (<code>docs/model_validering.html</code>) files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view the files as they were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/06caa240e2dd7b842de00bf8033a34959781d691/analysis/model_validering.rmd" target="_blank">06caa24</a>
</td>
<td>
GitHub
</td>
<td>
2021-05-09
</td>
<td>
Update model_validering.rmd
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/0e9dbb626e54a664cf9a26260e383bcf282c12d8/docs/model_validering.html" target="_blank">0e9dbb6</a>
</td>
<td>
GitHub
</td>
<td>
2021-05-09
</td>
<td>
Add files via upload
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/ca6b392cf67766ac06beb2c91ca769224ddb035d/analysis/model_validering.rmd" target="_blank">ca6b392</a>
</td>
<td>
GitHub
</td>
<td>
2021-05-09
</td>
<td>
Add files via upload
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/e98e355d470d9eb4c1d6123efff93d4a331d33af/analysis/model_validering.rmd" target="_blank">e98e355</a>
</td>
<td>
GitHub
</td>
<td>
2021-05-09
</td>
<td>
Merge branch ‘master’ into ny—malthe
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/e98e355d470d9eb4c1d6123efff93d4a331d33af/docs/model_validering.html" target="_blank">e98e355</a>
</td>
<td>
GitHub
</td>
<td>
2021-05-09
</td>
<td>
Merge branch ‘master’ into ny—malthe
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/b9ca680b6ba0281555cbead4ea79846b7c87d2b4/analysis/model_validering.rmd" target="_blank">b9ca680</a>
</td>
<td>
MHPHP
</td>
<td>
2021-05-09
</td>
<td>
docs files
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/b9ca680b6ba0281555cbead4ea79846b7c87d2b4/docs/model_validering.html" target="_blank">b9ca680</a>
</td>
<td>
MHPHP
</td>
<td>
2021-05-09
</td>
<td>
docs files
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/d483e95adf967c3b610248b20fc6e8eba46ff2d8/docs/model_validering.html" target="_blank">d483e95</a>
</td>
<td>
DitteM
</td>
<td>
2021-05-09
</td>
<td>
try
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/b012ce1afd4d28109ec191105d1c69e6143a681b/analysis/model_validering.rmd" target="_blank">b012ce1</a>
</td>
<td>
DitteM
</td>
<td>
2021-05-09
</td>
<td>
try
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/2f79d87cb0c08879b299c1edca0a0f7eee3315ae/docs/model_validering.html" target="_blank">2f79d87</a>
</td>
<td>
GitHub
</td>
<td>
2021-05-05
</td>
<td>
Add files via upload
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/8fa9526f9cb5ae3fd30dcf9402817e03daf64ef2/analysis/model_validering.rmd" target="_blank">8fa9526</a>
</td>
<td>
GitHub
</td>
<td>
2021-05-05
</td>
<td>
Add files via upload
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/17d057681c5a3c146fcaefae7128732f6491c69e/analysis/model_validering.rmd" target="_blank">17d0576</a>
</td>
<td>
MHPHP
</td>
<td>
2021-04-28
</td>
<td>
A lot of changed to model validation and added some plots to explroe where the low value p-vlaue are.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/17d057681c5a3c146fcaefae7128732f6491c69e/docs/model_validering.html" target="_blank">17d0576</a>
</td>
<td>
MHPHP
</td>
<td>
2021-04-28
</td>
<td>
A lot of changed to model validation and added some plots to explroe where the low value p-vlaue are.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/bb6ab8559f940dac2d89c46ee532a8ab75fa214c/analysis/model_validering.rmd" target="_blank">bb6ab85</a>
</td>
<td>
MHPHP
</td>
<td>
2021-04-21
</td>
<td>
Commited index again and changes in model_validering of the assumptions
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/bb6ab8559f940dac2d89c46ee532a8ab75fa214c/docs/model_validering.html" target="_blank">bb6ab85</a>
</td>
<td>
MHPHP
</td>
<td>
2021-04-21
</td>
<td>
Commited index again and changes in model_validering of the assumptions
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/8568f450ab91355243e8d27e4f53f89cdcaeedfb/analysis/model_validering.rmd" target="_blank">8568f45</a>
</td>
<td>
MHPHP
</td>
<td>
2021-04-19
</td>
<td>
HTML of model-validatoin
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/8568f450ab91355243e8d27e4f53f89cdcaeedfb/docs/model_validering.html" target="_blank">8568f45</a>
</td>
<td>
MHPHP
</td>
<td>
2021-04-19
</td>
<td>
HTML of model-validatoin
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/ae04bc6307c75a33266dbd1e55b0ff16b05603fb/analysis/model_validering.rmd" target="_blank">ae04bc6</a>
</td>
<td>
MHPHP
</td>
<td>
2021-04-19
</td>
<td>
Changes model_validering.rmd
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/nbendsen/DataProject/blob/6c0e88a7bfbd9c36e25d48491bbb1a309477d4b6/analysis/model_validering.rmd" target="_blank">6c0e88a</a>
</td>
<td>
GitHub
</td>
<td>
2021-04-15
</td>
<td>
Add files via upload
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/1bfbb028da6b72fa5c1e0c104b13bf880ea2e226/docs/model_validering.html" target="_blank">1bfbb02</a>
</td>
<td>
GitHub
</td>
<td>
2021-04-14
</td>
<td>
Add files via upload
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/nbendsen/DataProject/f1737141417ab177317f192f8bb619ec5fd6b00f/docs/model_validering.html" target="_blank">f173714</a>
</td>
<td>
GitHub
</td>
<td>
2021-04-14
</td>
<td>
Add files via upload
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<h2>
Introduction
</h2>
<p>On the home page we introduced the beta binomial update method. The method combines observed cover data from the small square in the center of the plot and the presence/absence data from the entire plot. This gave rise to a new updated dataset that could be used to give diversity estimates of the entire plot. This page is intended check the goodness-of-fit of the proposed method against the observed cover data.</p>
<p>When we created the method we assumed that it would describe the data well. Especially, we assumed that the diversity estimates we would get by using the beta binomial update method did not differ too much from the diversity estimates we would achieve by only using the observed cover data. However, this is only an assumption and not necessarily true. We did however assume that the diversity estimates would be a bit higher using the beta binomial update method instead of using the observed data, since we add a small value for species present in the plot, but not observed in the small square of the plot, which the cover data does not. One issue could be that in plots with low diversity, the prior distributions will change the data too much so that our model will estimate the diversity way too high compared to the diversity estimate we would get from the observed cover data of the small square. <br></p>
<p>The intention of this page is to validate whether this assumption is acceptable. It will be done by using the ideas of posterior predictive checks, which are presented below and further described in <span class="citation">[1]</span> and <span class="citation">[2]</span>.</p>
<h2>
Posterior predictive checks
</h2>
<p>In each plot we obtain a posterior distribution for each specie that have a <span class="math inline">\(1\)</span> in the corresponding presence/absence data of the plot. These posterior distributions are estimated using the observed cover data and observed presence/absence data.</p>
<p>For a given species in a plot the posterior distribution is given by</p>
<p><span class="math display">\[
\text{Posterior} \sim Beta(a+y,b+n-y)
\]</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the parameters from the estimated prior distribution for the species, <span class="math inline">\(y\)</span> is the number of hits by the species in the cover data and <span class="math inline">\(n\)</span> is the total number of possible hits for a species in a plot. In the case of the NOVANA dataset, we have <span class="math inline">\(n = 16\)</span>.</p>
<p>In our method we use the mean of the posterior distribution as an estimate of the species cover in the entire plot. Instead of using the mean for the validation we draw one sample from the posterior distribution for each species we know is present in the plot and use this as the estimate of the species cover in the entire the plot. We will refer to this as generated cover data. After the sampling we apply some test statistic of interest on the generated cover data for the plot. This test statistic should capture some of the aspects on the data we interested in. In our case we want the method to be used to estimate diversity, so using some sort of diversity estimator as test statistic is natural. We do this for all plots in the dataset.</p>
<p>We repeat the above process a 1000 times so that we get a distribution of generated test statistics for each plot. Lastly, we compare the test statistic we would get by using the observed cover data with the distributions of generated test statistics from the generated data. Again we do this for each plot. We refer to the test statistic we get from the observed cover data as the observed test statistic.</p>
<p>The idea behind posterior predictive checks is as follow: If the model assumption are appropriate, the generated data will look like the observed data viewed through the chosen test statistic. Escpecially, if we make a histogram of the generated test statistics the observed test statistic should not be an outlier. If it turns out that the observed test statistic is extreme compared to the generated test statistics it would cause some concern regarding whether the model is appropriate.</p>
<p>The posterior predictive check can be done visually as is done for the first 3 plots in the examples below. In addition to visual inspections of the histograms we can also calculate the tail-area probability, which we call the posterior predictive p-value [3]. If we let <span class="math inline">\(T(y)\)</span> be the observed test statistic and <span class="math inline">\(T(y^{rep})\)</span> be the distribution of generated test statistics, then we calculate the posterior predictive p-value as</p>
<p><span class="math display">\[
\text{posterior predictive p-value} = 2\cdot \min\Big(P(T(y^{rep}) \leq T(y)),P(T(y^{rep}) \geq T(y)) \Big)
\]</span></p>
<p>A small posterior predictive p-value close to zero indicates that the observed test statistic is not very likely relative to the generated test statistics and the posterior predictive check suggests that the model is misspecified with respect to the test statistic. The reason is, that when generating the test statistics and and forming the histogram, we observe a distribution for the generated test statistics. When the posterior predictive p-value is small, the observed test statistic will have a small probability of occurring in the generated distribution for the test statistics, hence will not be likely to occur and thereby is not similar to the generated test statistics. <br> As mentioned earlier we will use diversity estimates as test statistics. We saw on the home page that most plots in the beta binomial cover dataset had higher diversity estimates than in the observed dataset. However, that was not the case for all plots. We have defined the posterior predictive p-value as above to take into account when our method returns diversity estimate too high or too low relative to the observed dataset as we see both outcomes as extreme.</p>
<p>The posterior predictive checks are only intended to highlight if our model is likely given the data, i.e. the model fit the observed data well. This, however, does not mean that the model is “better” to estimate diversity than just using cover or presence/absence data. This is a further assumption we need to make based on considerations within the domain of ecology.</p>
<h2>
Model validation for 1. example
</h2>
<p>In the first example on the home page we worked with a subset of the NOVANA dataset from the tertiary habitat “Surt overdrev”. For further details on how this subset was created click <a href="data_6230.html">link</a>. In this subsection we will apply the ideas of posterior predictive checks to see how the beta binomial update method performs on this subset. <br></p>
<p>As test statistic we will use the Hill Simpson and Hill Shannon diversities that were also used in the example on the home page. Especially, we saw in the example on the home page how the diversity estimates obtained by the beta binomial cover dataset drifted further apart from the diversity estimates of the observed cover data when applying the the Hill Shannon diversity instead of the Hill Simpson diversity. Therefore, it is of interest to see how the properties of the two different ways to estimate diversity unfold when they are used as test statistics in our posterior predictive checks. Once again we define these as <span class="math display">\[
\text{Hill Simpson = } \frac{1}{\Sigma_{i=1}^S(p_i)^2}
\]</span> <span class="math display">\[
\text{Hill Shannon  = } e^{ - \Sigma_{i = 1}^Sp_i\cdot \ln(p_i)}
\]</span> Further details on the diversity estimates can be found on <a href="Diversity.html">link</a></p>
<p>We read in the datasets and remove the first 3 columns, since they do not contain information on species</p>
<pre class="r"><code>#Here we load the datasets for habitat 6230 in year 2014
cover &lt;- read.csv(&quot;data/cover_data_6230_year2014.csv&quot;)
freq &lt;- read.csv(&quot;data/frekvens_data_6230_year2014.csv&quot;)
#We remove the first 3 columns as they are not species
cover_data &lt;- cover[,4:ncol(cover)]
freq_data &lt;- freq[,4:ncol(freq)]</code></pre>
<p>With the data we make a visual inspection of the first 3 plots where the Hill Shannon diversity is used as a test statistic. A description of the code implementation of the ppc()-function can be found in the section “Code implementation” <a href="model_val.R">link</a>.</p>
<pre class="r"><code>ppc(1, freq_data, cover_data)</code></pre>
<p><img src="figure/model_validering.rmd/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ppc(2, freq_data, cover_data)</code></pre>
<p><img src="figure/model_validering.rmd/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ppc(3, freq_data, cover_data)</code></pre>
<p><img src="figure/model_validering.rmd/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>All posterior predictive p-values are big in the above histograms which suggests that the model is appropriate for the first three plots viewed through the Hill Shannon diversity. If we run the posterior predictive check on all plots we can get the proportion of posterior predictive p-values that are less than 0.05. We use a posterior predictive p-value of 0.05 as a threshold for when to say that the observed test statistic is not very likely given the model.</p>
<pre class="r"><code>p_values_hill_shannon &lt;- read.csv(&quot;data/hill_shannon_pval2.csv&quot;)
sum(p_values_hill_shannon &lt; 0.05) / nrow(cover)</code></pre>
<pre><code>[1] 0.07873377</code></pre>
<p>The interpretation is that for around 8% of the plots the posterior predictive check suggests that the beta binomial update method is not appropriate, i.e. the observed Hill Shannon diversity is an outlier compared to the Hill Shannon diversities that the model would generate.</p>
<p>However, it is important to check if these plots are randomly placed or lumped together for either high or low diversities. To do this we plot the mean of all the generated Hill Shannon diversities for each plot against the observed Hill Shannon diversity for the corresponding plot. We color each point in the scatter plot to visualize the posterior predictive p-value of the plot</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /> It is worth to notice that it is specially plots with low diversity that our model seems to give diversity estimates that differ a lot from what we would get from the observed cover data. However, this does not necessarily mean that our model is bad. What it does mean is that it is more likely for these plots that the beta binomial update method overestimate the diversity for the entire plot relative to the observed cover data of the small square. Otherwise the beta binomial update method seems to generate data that is aligned with the observed cover data.</p>
<p>Lastly, we will create a similar plot but with the Hill Simpson diversity used as test statistic. In the example on the home page we saw that the Hill Simpson diversity was the diversity estimator that gave a diversity estimate that differed the least between the beta binomial updated cover data and the observed cover data.</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Again we see that it is only for plots with low diversity where the observed Hill Simpson diversity seems unlikely given the model. However, this is only a very small proportion.</p>
<pre class="r"><code>sum(p_values_hill_simpson$p_values &lt; 0.05)/nrow(cover)</code></pre>
<pre><code>[1] 0.01136364</code></pre>
<p>This shows that when the Hill Simpson diversity is used as test statistic the model will generate data that looks very much like the observed data. Much more than the case was when we used the Hill Shannon diversity as test statistic.</p>
<p>This matches with what we found in the “Hill Diversity comparison” section on the home page. There we also saw that the beta binomial update method has less importance viewed through the Hill Simpson diversity than through the Hill Shannon diversity.</p>
<p>For both test statistics the conclusion is that the model mostly generated data that looks like the observed data. This gives confidence in the fact that the model has captured some good aspects of the observed data and gives sensible and thereby useful result to work with. Again it should be emphasized that it does not mean the the model gives better diversity estimates then what could be obtained by only using the observed cover data.</p>
<h2>
Model validation for 2. example
</h2>
<p>We also want to include the posterior predictive checks we get when we apply these to the data from tertiary habitat “Kalkoverdrev” from year 2009 that we used in our second example on the home page. We make similar plots as we did above. First with the Hill Shannon diversity as test statistic and then with the Hill Simpson diveristy as test statistic.</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Again we are interested in the proportion of plots where the observed test statistic is an outlier compared to the distribution of generated test statistics. For the Hill Shannon diversity the proportion is almost 13% if we again use a posterior predictive p-value of 0.05 as a threshold</p>
<pre class="r"><code>sum(p_values$p_values &lt; 0.05)/nrow(cover_6210)</code></pre>
<pre><code>[1] 0.1278254</code></pre>
<p>And the proportion of plots where the observed Hill Simpson diversity is an outlier relative to the generated Hill Simpson diversities is only around 2%</p>
<pre class="r"><code>sum(p_values_simpson$p_values &lt; 0.05)/nrow(cover_6210)</code></pre>
<pre><code>[1] 0.020265</code></pre>
<p>In the case of both test statistics we see that a larger proportion of the plots have posterior predictive p-values indicating that the observed test statistic is unlikely under the model than in “surt overdrev” (first example). As an example, in “surt overdrev” the proportion of posterior predictive p-values below 0.05 with the Hill Shannon used as test statistic was only around 8%. However, the proportion of plots is still not so big that it causes concerns regarding the whether the assumptions underlying the model are appropriate.</p>
<p>Below we study if there are some differences in the data from “surt overdrev” and “kalkoverdrev” that could explain why the model perform worse for the “kalkoverdrev” in these posterior predictive checks.</p>
<h2>
Comparison of tertiary habitat types
</h2>
<p>First of all, we look at some statistics to compare the plots in “kalk overdrev” (6210 in NOVANA) and the plots in “surt overdrev” (6230 in NOVANA). If we compare the two tertiary habitat types we see that the number of species in the habitat type “kalk overdrev” is generally higher than in the habitat type “surt overdrev” at plot level. <br> We look at the presence/absence data for the “surt overdrev” and “kalk overdrev” respectively. For each dataset we sum the number of present species in each plot and take the mean of all these. In the presence/absence data for “kalk overdrev” the mean number of species in a plot is 32.6 species while it is 26.8 species in the presence/absence data for “surt overdrev”. <br></p>
<p>We do the same with the cover data for both datasets respectively, i.e. we sum the number of species in a plot if the species is hit by at least one pin and take the mean over all plots. In the observed cover data for “kalkoverdrev” the average observed number of species in a plot is 10.5 while it is 8.3. in the observed cover data for “surt overdrev”. <br></p>
<p>What is really interesting is the the proportion of a plot where a species has a <span class="math inline">\(1\)</span> in the presence/absence data and at least one observation in the corresponding cover data. If this proportion is low we suspect these plots to be the ones where the beta binomial update method has the biggest impact. For each plot we take the number of species found in the cover data and divide this by the number of species found in the presence/absence data for that plot. A species is found in the cover data for a plot if it is hit be at least one pin. If we again just take the average we find that the average proportion for each plot in “kalk overdrev” is 0.33 and the average proportion for each plot in “surt overdrev” is 0.32. Again there are no big differences in the statistics between the two tertiary habitat types. <br><br> However, we want to study this last statistic a bit further. To visualize the distribution of the proportions from the plots in each of the two tertiary habitat types we make the following histograms</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>There are no big differences between the two histograms. The most important thing to notice from this histogram is that the histogram for “kalk overdrev” has more plots with a very low proportion. In “kalkoverdrev” 1.9% of the plots have proportion below 0.1 while this is only the case for 1.2% of the plots in “surt overdrev”. <br></p>
<p>We suspect that the beta binomial update method has most impact on these plots with low proportion. To illustrate this point we plot posterior predictive p-value for each plot against the proportion of a plot where species are found in both cover data and presence/absence data. The posterior predictive p-values come from the posterior predictive checks where the Hill Shannon was used as test statistic. The y-axis is -log(posterior predictive p-value) and the dashed line is equal to -log(0.05) for comparison. Because we have taken the negative logarithm on the posterior predictive p-value the plots above the dashed line are more extreme.</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The above scatterplots nicely visualize the fact that plots with a low proportion are more sensitive to the beta binomial update method. We see that when a plot has a low proportion the observed Hill Shannon diversity is far more likely to be different than the generated Hill Shannon diversities we get from the beta binomial update method. Furthermore, we notice that all plots with a posterior predictive p-value less than 0.05 have a proportion less than 0.34. This fits nicely with what we expected and is probably the reason that our method has a slightly bigger effect on “kalk overdrev” than “surt overdrev” as “kalk overdrev” has a bit more plots where the proportion of species that are found in the presence/absence data are also found in the cover data is very low.</p>
<h2>
Other validations
</h2>
<p>By using the posterior predictive checks we studied if the observed data is a likely outcome of our constructed model. It turned out to be the conclusion in most cases. However, the observed data is only a sample for each plot. This means that the “true” diversity of a plot might be rather different than the information we get from the observed cover and presence/absence data. The more correct diversity estimate could be obtained if we had cover data for the entire plot and not only the small square in the center of the plot, this will however be a huge job to collect. The beta binomial update method that we have created should in theory help to give a better diversity estimate based on the sample from the cover data in the small square and the corresponding presence/absence data of the entire plot, since we include all species present in the entire plot.</p>
<p>An interesting thing to do would be to simulate a dataset (both cover and present/absence) with known diversities for the plots in the dataset. When knowing the true diversity we could have tested how much better our method would be to estimate the diversity rather than only using the cover or presence/absence data. However, this validation of the method has not been possible within the scope of this project and is a topic for further studies in the future. <br><br></p>
<h2>
Code implementation
</h2>
<p>The following is an implementation of the ppc() function that was used in the make visual inspections of the posterior predictive checks for some plots.</p>
<pre class="r"><code>library(fitdistrplus)</code></pre>
<pre class="r"><code>#We read the cover data and the presence/absense data without the first 4 columns, as they do not cotains information on species
cover_data &lt;- cover[,4:ncol(cover)]
freq_data &lt;- freq[,4:ncol(freq)]
  
#We make a dataframa for the parameters of the prior distribution for each plot, so it is possible to save the parameteres, and not calculate them when making the posteriror for each plot. Each row will contain the number/name of the specie and its corresponding parameters for the prior 
beta_fit &lt;- data.frame(matrix(ncol = 3, nrow = 0))
# We name the columns in the 
colnames(beta_fit) &lt;- c(&quot;species&quot;,&quot;a&quot;, &quot;b&quot;)</code></pre>
<pre class="r"><code>#Here we calculate the parameters for the priror distributions of each specie:
for (specie in colnames(cover_data)) {
  #First we normalise. Since there is in total used 16 pins for each plot, we will devide the entries in the cover data by 16
  beta_data &lt;- cover_data[,specie]/16
  
  #Now we remove the plots where the specie is not present. This can be done by using the information from the presence, absense data. If it        contains a 1, then the specie is present in the plot, if 0 it is absent. 
  beta_data &lt;- beta_data[freq_data[[specie]] == 1]
    
  #If the specie is not present in any of the plots, we do not have information to make a prior distribution for it, and will just give it parameters a=0 and b=0 as seen in the else clause.
  if (length(unique(beta_data)) &gt; 1) {
    #We use the method of moments to fit the prior beta distribution
    beta_data_fitted &lt;- fitdist(beta_data, &quot;beta&quot;, method = &quot;mme&quot;)
    
    #The parameters are added to the dataframe
    beta_fit[nrow(beta_fit) + 1,] &lt;- c(specie, beta_data_fitted$estimate[1], beta_data_fitted$estimate[2])
      
  }
  else {
    beta_fit[nrow(beta_fit) + 1,] &lt;- c(specie, 0,0)
    
  }
}</code></pre>
<pre class="r"><code>#n is the row number of the plot we are working with
n &lt;- 1
#We define which species are in present in the plot
species_spotted_in_frekvens &lt;- colnames(freq_data[c(freq_data[n,]  == 1)])
#We define which species are present in the present/absent dataset but are not seen in the cover dataset
not_in_cover &lt;- setdiff(species_spotted_in_frekvens,colnames(cover_data))
#We remove the species, that at present in the plots in the present/absent data, but are not observed in any plots in the cover data
species_spotted_in_frekvens &lt;- setdiff(species_spotted_in_frekvens, not_in_cover)
# we remove the columns, that are not representing species
observed &lt;- cover_data[n,c(species_spotted_in_frekvens)]
tmp &lt;- observed[observed &gt; 0]
T_static &lt;- -sum(tmp/sum(observed) * log((tmp/sum(observed))))
#We make a dataframe to save the parameters of the posterior for each spotted specie in the plot
new_beta &lt;- data.frame(matrix(ncol = 3, nrow = 0))
  
colnames(new_beta) &lt;- c(&quot;species&quot;,&quot;a&quot;, &quot;b&quot;) 
for (species_spotted in species_spotted_in_frekvens ) {
      
  #We define the parameters for the posterior 
      alpha_post &lt;- as.numeric(beta_fit[beta_fit$species == species_spotted,]$a) + cover_data[[species_spotted]][n] 
    beta_post &lt;-  as.numeric(beta_fit[beta_fit$species == species_spotted,]$b) + 16 - cover_data[[species_spotted]][n]
    
      #If the parameters are to small, we change them to 0, since R has a hard time working with them
      alpha_post &lt;- ifelse(alpha_post &lt; 1e-10, 0, alpha_post)
      beta_post &lt;- ifelse(beta_post &lt; 1e-10,0, beta_post)
      
      #We add the parameters to the dataframe
      new_beta[nrow(new_beta) + 1,] &lt;- c(species_spotted, alpha_post, beta_post)}
#We make a vector, to save the shannon indexes produces in each iteration
shannon &lt;- c()
for (i in 1:1000){
  
  #Vector for saving the random generated values from the posterior of each specie
  values &lt;- c()
  for (ele in species_spotted_in_frekvens){
    #These are the parameters, for the posterior of the specie
    a &lt;- as.numeric(new_beta[new_beta$species == ele,]$a)
    b &lt;- as.numeric(new_beta[new_beta$species ==ele,]$b)
    
    # We draw a random number from a beta distribution with the parameters for that specie and add it to the vector
    values &lt;- c(values, rbeta(1,a,b))
    
    
  }
  #We remove the values that are to small
  tmp &lt;- values[ values &gt; 0.00001]
  total &lt;- sum(tmp)
  
  #We calculate the Hill shannon diversity
  shannon &lt;- c(shannon,exp(-sum(tmp/total * log((tmp/total)))))
  
}
min_val &lt;- min(T_static, min(shannon)) - 0.1
max_val &lt;- max(T_static, max(shannon)) + 0.1
n &lt;- length(shannon)
pvalue &lt;- 2*min(sum(shannon&gt;= T_static)/n, sum(shannon&lt;= T_static)/n)
#This is the code that produces the histogram over the generated data
hist(shannon, xlim = c(min_val, max_val), main = sprintf(&quot;Histogram of simulated Hill Shannon diversities for plot %d&quot;, n), xlab = &quot;Hill Shannon diversity&quot;)
legend(&quot;topright&quot;, legend = &quot;Red line is observed \n Hill Shannon index&quot;)
abline(v = T_static, col = &quot;red&quot; )</code></pre>
<h2>
References
</h2>
<div id="refs" class="references csl-bib-body">
<div id="ref-MimnoE3441" class="csl-entry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">Mimno, D.; Blei, D.M. and Engelhardt, B.E.: Posterior predictive checks to quantify lack-of-fit in admixture models of latent population structure, Proceedings of the National Academy of Sciences, <strong>112</strong> (2015), no. 26, pp. E3441–E3450.</div>
</div>
<div id="ref-Gelman" class="csl-entry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Gelman, A.; Meng, X.-L. and Stern, H.: POSTERIOR PREDICTIVE ASSESSMENT OF MODEL FITNESS VIA REALIZED DISCREPANCIES, Statistica Sinica, <strong>6</strong> (1996), no. 4, pp. 733–760.</div>
</div>
</div>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 4.0.3 (2020-10-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 19042)

Matrix products: default

locale:
[1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252   
[3] LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C                   
[5] LC_TIME=Danish_Denmark.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] fitdistrplus_1.1-3 forcats_0.5.0      stringr_1.4.0      dplyr_1.0.2       
 [5] purrr_0.3.4        readr_1.4.0        tidyr_1.1.2        tibble_3.0.4      
 [9] ggplot2_3.3.2      tidyverse_1.3.0    survival_3.2-7     MASS_7.3-53       

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.5       lubridate_1.7.9  lattice_0.20-41  assertthat_0.2.1
 [5] rprojroot_1.3-2  digest_0.6.27    R6_2.5.0         cellranger_1.1.0
 [9] backports_1.1.10 reprex_0.3.0     evaluate_0.14    httr_1.4.2      
[13] pillar_1.4.6     rlang_0.4.8      readxl_1.3.1     rstudioapi_0.11 
[17] whisker_0.4      blob_1.2.1       Matrix_1.2-18    rmarkdown_2.5   
[21] labeling_0.4.2   splines_4.0.3    munsell_0.5.0    broom_0.7.2     
[25] compiler_4.0.3   httpuv_1.5.5     modelr_0.1.8     xfun_0.18       
[29] pkgconfig_2.0.3  mgcv_1.8-33      htmltools_0.5.0  tidyselect_1.1.0
[33] workflowr_1.6.2  fansi_0.4.1      crayon_1.3.4     dbplyr_1.4.4    
[37] withr_2.3.0      later_1.1.0.1    grid_4.0.3       nlme_3.1-149    
[41] jsonlite_1.7.1   gtable_0.3.0     lifecycle_0.2.0  DBI_1.1.0       
[45] git2r_0.28.0     magrittr_1.5     scales_1.1.1     cli_2.1.0       
[49] stringi_1.5.3    farver_2.0.3     fs_1.5.0         promises_1.2.0.1
[53] xml2_1.3.2       ellipsis_0.3.1   generics_0.1.0   vctrs_0.3.4     
[57] tools_4.0.3      glue_1.4.2       hms_0.5.3        yaml_2.2.1      
[61] colorspace_1.4-1 rvest_0.3.6      knitr_1.30       haven_2.3.1     </code></pre>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>




</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
