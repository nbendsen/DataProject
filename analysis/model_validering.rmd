---
title: "Model validation"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
    css: style.css
bibliography: references.bib
csl: brewingscience.csl
editor_options:
  chunk_output_type: console
---

---
title: "Model validation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(MASS)
library(survival)
library(tidyverse)
library(ggplot2)
```

<br>

On [Bayesian model](Model.html) we introduced the beta binomial cover update method. The method combines observed cover data from the small square in the center of the plot with the presence/absence data from the entire plot. This gave rise to a new updated dataset with estimates of cover data for the entire plot for each species present.<br>

For all species present in a plot, we update the abundance estimate in the cover data as we assume that this will lead to a more accurate estimate of the species diversity within the plot. We assume that the diversity estimates we get by using the beta binomial cover update method do not differ too much from the diversity estimates we obtain by only using the observed cover data. This is however only an assumption and not necessarily true. <br>

One important assumption for the model is that species within the same tertiary habitat will follow the same spatially aggregated distribution. This assumption is used to develop the prior distribution for each species. <br>

The intention of this page is to check whether the diversity estimates obtained using the data from the beta binomial cover update model is somewhat similar to those obtained using the cover data. It will be done by using the ideas of posterior predictive checks which are presented below and further described in [@MimnoE3441] and [@Gelman].



<h2>Posterior predictive checks</h2>
In each plot we obtain a posterior distribution for each species that have a $1$ in the corresponding presence/absence data of the plot. These posterior distributions are estimated using the observed cover data and observed presence/absence data.

For a given species in a plot the posterior distribution is given by 

$$
\theta|(Y = y) \sim Beta(a+y,b+n-y)
$$
where $a$ and $b$ are the parameters from the estimated prior distribution for the species and $y$ is the number of pins the species is hit by in the cover data out of $n$ possible. In the case of the NOVANA dataset we have $n = 16$.


In our model we use the mean of the posterior distribution as an estimate of the species cover in the entire plot. When we apply the posterior predictive checks we draw one sample from the posterior distribution for each species present in the plot and use this as the estimate of the species relative cover in the entire the plot. We will refer to this as generated cover data. Next, we apply some test statistic of interest on the generated cover data for the plot. This test statistic should capture some aspects of the data we are interested in. In our case we want the method to be used to estimate species diversity, so using some sort of diversity estimator as test statistic is natural. We do this for all plots in the dataset.

We repeat the above process 1000 times so that we get a distribution of generated test statistics for each plot. Lastly, we compare the test statistic we would get by using the observed cover data for a plot with the distribution of generated test statistics for the same plot. Again we do this for each plot. We refer to the test statistic we get from the observed cover data as the observed test statistic. 


The idea behind posterior predictive checks is as follows: If the model assumptions are appropriate the generated data will look like the observed data viewed through the chosen test statistic. Especially, if we make a histogram of the generated test statistics the observed test statistic should not be an outlier. If it turns out that the observed test statistic is extreme compared to the generated test statistics it would cause some concern regarding whether the model is appropriate.

The posterior predictive check can be done visually as is done for the first 3 plots in the examples below.
In addition to visual inspections of the histograms we can also calculate the tail-area probability, which we call the posterior predictive p-value [3]. If we let $T(y)$ be the observed test statistic and $T(y^{rep})$ be the distribution of generated test statistics, then we calculate the posterior predictive p-value as

$$
\text{posterior predictive p-value} = 2\cdot \min\Big(P(T(y^{rep}) \leq T(y)),P(T(y^{rep}) \geq T(y)) \Big)
$$

A small posterior predictive p-value close to zero indicates that the observed test statistic is not very likely relative to the generated test statistics and the posterior predictive check suggests that the model is misspecified with respect to the test statistic.
<br>
As mentioned earlier we will use diversity estimates as test statistics. We saw on [Example](example.html) that most plots in the beta binomial cover dataset had higher diversity estimates than in the observed dataset. However, that was not the case for all plots. We have defined the posterior predictive p-value as above to take into account when our method returns diversity estimates too high or too low relative to the observed test statistic as we see both outcomes as extreme. 

The posterior predictive checks are only intended to highlight if our model is likely given the data, i.e. the model generates data that is close to the observed data. This, however, does not mean that the model is "better" to estimate diversity than just using cover or presence/absence data. That is a further assumption we need to make based on considerations within the domain of ecology. 

<h2> Model validation for "Nardus grasslands" </h2>
In the two examples *"Comparing diversity measures"* and *"Species diversity and nitrogen deposition"* on [Example](example.html) we worked with a subset of the NOVANA dataset from the tertiary habitat "Nardus grasslands". Further details on how this subset was created can be seen here [data 6230](data_6230.html). In this subsection we will apply the ideas of posterior predictive checks to see how the beta binomial cover update method performs on this subset.
<br>

As test statistic we will use the Hill Simpson and Hill Shannon diversities that were also used on [Example](example.html). Especially, we saw in the *"Comparing diversity measures"* how the diversity estimates obtained by the beta binomial cover data drifted further apart from the diversity estimates of the observed cover data when applying the the Hill Shannon diversity instead of the Hill Simpson diversity. Therefore, it is of interest to see how the properties of the two different ways to estimate diversity unfold when they are used as test statistics in our posterior predictive checks. Once again we define these as 
$$
\text{Hill Simpson = } \frac{1}{\Sigma_{i=1}^S(p_i)^2}
$$
$$
\text{Hill Shannon  = } e^{ - \Sigma_{i = 1}^Sp_i\cdot \ln(p_i)}
$$

Further details on the diversity estimates can be found on [Diversity](Diversity.html).

We read in the datasets and remove the first 3 columns since they do not contain information on species.
```{r}
#Here we load the datasets for habitat 6230 in year 2014
cover <- read.csv("data/cover_data_6230_year2014.csv")
freq <- read.csv("data/frekvens_data_6230_year2014.csv")
#We remove the first 3 columns as they are not species
cover_data <- cover[,4:ncol(cover)]
freq_data <- freq[,4:ncol(freq)]
```

With the data we make a visual inspection of the first 3 plots where the Hill Shannon diversity is used as a test statistic. A description of the code implementation of the `ppc()` function can be found in [Code implementation](ppc_code.html). 

```{r, include= FALSE}
#We load the function for used for the validation
source("code/model_val.R")
source("code/function.R")
```

```{r, warning=FALSE}
ppc(1, freq_data, cover_data)
```

```{r, warning=FALSE}
ppc(2, freq_data, cover_data)
```

```{r, warning=FALSE}
ppc(3, freq_data, cover_data)
```

All posterior predictive p-values are big in the above histograms which suggests that the model is appropriate for the first three plots viewed through the Hill Shannon diversity. If we run the posterior predictive check on all plots we can get the proportion of posterior predictive p-values that are less than 0.05. We use a posterior predictive p-value of 0.05 as a threshold for when to say that the observed test statistic is not very likely given the model.

```{r}
p_values_hill_shannon <- read.csv("data/hill_shannon_pval2.csv")
sum(p_values_hill_shannon < 0.05) / nrow(cover)
```

The interpretation is that for around 8% of the plots the posterior predictive check suggests that the beta binomial cover update method is not appropriate, i.e. the observed Hill Shannon diversity is an outlier compared to the Hill Shannon diversities that the model would generate.

However, it is important to check if these plots are randomly placed or lumped together for either high or low diversities. To do this we plot the mean of the generated Hill Shannon diversities for each plot against the observed Hill Shannon diversity for the same plot. We color each point in the scatter plot to visualize the posterior predictive p-value of the plot.

```{r, include = FALSE}
generated_hill_shannon <- read.csv("data/hill_shannon.csv")[,1:1000]
observed_hill_shannon <- hill_shannon(cover_data)
colnames(p_values_hill_shannon) <- "p_values"
p_values_hill_shannon$mean <- rowMeans(generated_hill_shannon)
p_values_hill_shannon$observed <- observed_hill_shannon

b <- max(p_values_hill_shannon$p_values)
p_values_hill_shannon$p_values <- ifelse(p_values_hill_shannon$p_values==b, 1, p_values_hill_shannon$p_values)
```


```{r, echo = FALSE}
ggplot(data = p_values_hill_shannon, mapping = aes(x = observed, y = mean)) +
  geom_point(aes(col = p_values))+
  ylab("Mean of generated Hill Shannon diversity")+
  xlab("Hill Shannon diversity of observed cover data") +
  ggtitle("Mean of generated Hill Shannon diversity vs observed Hill Shannon diversity") +
  geom_abline()+
  scale_color_gradient(low="blue", high="red")+
  labs(color = "Posterior predictive p-value")
```

It is worth to notice that it is especially plots with low diversity that our model seems to give diversity estimates that differ a lot from what we would get from the observed cover data. However, this does not necessarily mean that our model is bad. What it does mean is that it is more likely for these plots that the beta binomial cover update method overestimate the diversity for the entire plot relative to the observed cover data of the small square. Otherwise the beta binomial cover update method seems to generate data that is aligned with the observed cover data.

Lastly, we will create a similar plot but with the Hill Simpson diversity used as test statistic. On the *"Comparing diversity measures"* section on [Example](example.html) we saw that the Hill Simpson diversity was the diversity estimator that gave a diversity estimate that differed the least between the beta binomial cover data and the observed cover data. 


```{r, include = FALSE}
generated_hill_simpson <- read.csv("data/hill_simpson.csv")[,1:1000]
p_values_hill_simpson <- read.csv("data/hill_simpson_pval2.csv")
observed_hill_simpson <- hill_simpson(cover_data)
colnames(p_values_hill_simpson) <- "p_values"
p_values_hill_simpson$mean <- rowMeans(generated_hill_simpson)
p_values_hill_simpson$observed <- observed_hill_simpson

a <- max(p_values_hill_simpson$p_values)
p_values_hill_simpson$p_values <- ifelse(p_values_hill_simpson$p_values==a, 1, p_values_hill_simpson$p_values)
```


```{r, echo = FALSE}
ggplot(data = p_values_hill_simpson, mapping = aes(x = observed, y = mean )) +
  geom_point(mapping = aes(col = p_values))+
  ylab("Mean of generated Hill Simpson diversity")+
  xlab("Hill Simpson diversity of observed cover data") +
  ggtitle("Mean of generated Hill Simpson diversity vs observed Hill Simpson diversity") +
  geom_abline()+
  scale_color_gradient(low="blue", high="red")+
  labs(color = "Posterior predictive p-value")
```

Again we see that it is only for plots with low diversity where the observed Hill Simpson diversity seems unlikely given the model. However, this is only a very small proportion.

```{r}
sum(p_values_hill_simpson$p_values < 0.05)/nrow(cover)
```

This shows that when the Hill Simpson diversity is used as test statistic the model will generate data that looks very much like the observed data. Way more than the case was when we used the Hill Shannon diversity as test statistic.

This matches with what we found in the "Hill Diversity comparison" section on the example page under *Comparing diversity measures*. There we also saw that the beta binomial cover update method has less importance viewed through the Hill Simpson diversity than through the Hill Shannon diversity.

For both test statistics the conclusion is that the model mostly generated data that looks like the observed data. This gives confidence in the fact that the model has captured some good aspects of the observed data and gives sensible and thereby useful result to work with. Again it should be emphasized that it does not mean the the model gives better diversity estimates than what could be obtained by only using the observed cover data.

<h2>Model validation for "Dry calcareous grasslands" example </h2>
We also want to apply the posterior predictive checks to the data from tertiary habitat "Dry calcareous grasslands" from year 2009. This was the other subset of the NOVANA data that we used on [Examples](example.html). We make similar plots as we did above. First with the Hill Shannon diversity as test statistic and then with the Hill Simpson diversity as test statistic.


```{r, include = FALSE}
cover_6210 <- read.csv("data/cover_data_6210_year2009.csv")
generated_hill_shannon6210 <- read.csv("data/hill_shannon_6210.csv")[,1:1000]
observed_hill_shannon6210 <- hill_shannon(cover_6210, 3)
p_values <- read.csv("data/6210_hill_shannon_pvalues2.csv")
colnames(p_values) <- "p_values"
p_values$mean <- rowMeans(generated_hill_shannon6210)
p_values$observed <- observed_hill_shannon6210
p_values$p_values <- ifelse(p_values$p_values>1, 1,p_values$p_values)
```




```{r, include = FALSE}
cover_6210 <- read.csv("data/cover_data_6210_year2009.csv")
generated_hill_simpson6210 <- read.csv("data/hill_simpson_6210.csv")[,1:1000]
observed_hill_simpson6210 <- hill_simpson(cover_6210, 3)
p_values_simpson <- read.csv("data/6210_hill_simpson_pvalues2.csv")
colnames(p_values_simpson) <- "p_values"
p_values_simpson$mean <- rowMeans(generated_hill_simpson6210)
p_values_simpson$observed <- observed_hill_simpson6210
```



```{r, echo = FALSE}
ggplot(data = p_values, mapping = aes(x = observed, y = mean)) +
  geom_point(aes(col = p_values))+
  ylab("Mean of generated Hill Shannon diversity")+
  xlab("Hill Shannon diversity of observed cover data") +
  ggtitle("Mean of generated Hill Shannon diversity vs observed Hill Shannon diversity") +
  geom_abline() +
  scale_color_gradient(low="blue", high="red")+
  labs(color = "Posterior predictive p-value")
```

```{r, echo = FALSE}
ggplot(data = p_values_simpson, mapping = aes(x = observed, y = mean )) +
  geom_point(mapping = aes(col = p_values ))+
  ylab("Mean of generated Hill Simpson diversity")+
  xlab("Hill Simpson diversity of observed cover data") +
  ggtitle("Mean of generated Hill Simpson diversity vs observed Hill Simpson diversity") +
  geom_abline()+
  scale_color_gradient(low="blue", high="red", guide = "colourbar")+
  labs(color = "Posterior predictive p-value")
```

Again we are interested in the proportion of plots where the observed test statistic is an outlier compared to the distribution of generated test statistics. For the Hill Shannon diversity the proportion is almost 13% if we again use a posterior predictive p-value of 0.05 as a threshold.

```{r}
sum(p_values$p_values < 0.05)/nrow(cover_6210)
```

And the proportion of plots where the observed Hill Simpson diversity is an outlier relative to the generated Hill Simpson diversities is only around 2%.

```{r}
sum(p_values_simpson$p_values < 0.05)/nrow(cover_6210)
```

In the case of both test statistics we see that a larger proportion of the plots have posterior predictive p-values indicating that the observed test statistic is more unlikely under the model than in "Nardus grasslands" (first example). As an example, in "Nardus grasslands" the proportion of posterior predictive p-values below 0.05 with the Hill Shannon used as test statistic was only around 8%. However, the proportion of plots is still not so big that it causes concerns regarding the whether the assumptions underlying the model are appropriate. Especially if we take into account that it is again plots with a small diversity that have a low posterior predictive p-value. This is in line with what we expected from the model.

Below we study if there are some differences in the data between "Nardus grasslands" and "Dry calcareous grasslands" that could explain why the model lead to more plots with extreme posterior predictive p-values for the "Dry calcareous grasslands".

<h2> Comparison of tertiary habitat types </h2>

```{r, include= FALSE}
freq_6210 <- read.csv("data/frekvens_data_6210_year2009.csv")
```

```{r, include = FALSE}
stats_6230 <- cover[1:3]
stats_6210 <- cover_6210[1:3]
stats_6230$freq_sum_6230 <- rowSums(freq[4:ncol(freq)])
stats_6210$freq_sum_6210 <- rowSums(freq_6210[4:ncol(freq_6210)])
mean_freq_6230 <- mean(stats_6230$freq_sum_6230)
mean_freq_6210 <- mean(stats_6210$freq_sum_6210)
stats_6230$cover_sum_6230 <- rowSums(cover[4:ncol(cover)] >  0)
stats_6210$cover_sum_6210 <- rowSums(cover_6210[4:ncol(cover_6210)] > 0 )
stats_6230$cover_mean_over_0_6230 <- rowMeans(replace(cover[4:ncol(cover)], cover[4:ncol(cover)] == 0, NA), na.rm = TRUE)
stats_6210$cover_mean_over_0_6210<- rowMeans(replace(cover_6210[4:ncol(cover_6210)], cover_6210[4:ncol(cover_6210)] == 0, NA), na.rm = TRUE)
mean_cover_mean_over_0_6230 <- mean(stats_6230$cover_mean_over_0_6230)
mean_cover_mean_over_0_6210 <- mean(stats_6210$cover_mean_over_0_6210)
mean_cover_6230 <- mean(stats_6230$cover_sum_6230)
mean_cover_6210 <- mean(stats_6210$cover_sum_6210)
stats_6230$difference_6230 <- stats_6230$freq_sum_6230 - stats_6230$cover_sum_6230
stats_6210$difference_6210 <- stats_6210$freq_sum_6210 - stats_6210$cover_sum_6210
stats_6230$proportion_6230 <- stats_6230$cover_sum_6230 / stats_6230$freq_sum_6230 
stats_6210$proportion_6210 <- stats_6210$cover_sum_6210 / stats_6210$freq_sum_6210
mean_proportion_6230 <- mean(stats_6230$proportion_6230)
mean_proportion_6210 <- mean(stats_6210$proportion_6210)
stats_6210$pval <- p_values$p_values
stats_6230$pval <- p_values_hill_shannon$p_values
```

```{r, include = FALSE}
mean(p_values_hill_shannon$observed[p_values_hill_shannon$p_values < 0.05])
mean(p_values$observed[p_values$p_values < 0.05])
```

```{r, include = FALSE}
mean(p_values_hill_shannon$observed)
mean(p_values$observed)
```


First of all, we look at some statistics to compare the plots in "Dry calcareous grasslands" and the plots in "Nardus grasslands". If we compare the two tertiary habitat types we see that the number of species in the habitat type "Dry calcareous grasslands" is generally higher than in the habitat type "Nardus grasslands"  at plot level. 
<br>
We look at the presence/absence data for the "Nardus grasslands" and "Dry calcareous grasslands" respectively. For each dataset we sum the number of present species in each plot and take the mean of all these. In the presence/absence data for "Dry calcareous grasslands" the mean number of species in a plot is 32.6 species while it is 26.8 species in the presence/absence data for "Nardus grasslands".
<br>

We do the same with the cover data for both datasets respectively, i.e. we sum the number of species spotted in the cover data for a plot. A species is spotted in the cover data if it is hit by at least one pin. Again we take the mean over all plots. In the observed cover data for "Dry calcareous grasslands" the average number of species spotted in a plot is 10.5 while it is 8.3. in the observed cover data for "Nardus grasslands".
<br>

What is really interesting is the the proportion of a plot where a species has a $1$ in the presence/absence data and at least one observation in the corresponding cover data. If this proportion is low  we suspect these plots to be the ones where the beta binomial cover update method has the biggest impact. For each plot we take the number of species found in the cover data and divide this by the number of species found in the presence/absence data for that plot. A species is found in the cover data for a plot if it is hit be at least one pin. If we again just take the average we find that the average proportion for each plot in "Dry calcareous grasslands" is 0.33 and the average proportion for each plot in "Nardus grasslands" is 0.32. Again there are no big differences in the statistics between the two tertiary habitat types. 
<br><br>
However, we want to study this last statistic a bit further. To visualize the distribution of the proportions from the plots in each of the two tertiary habitat types we make the following histograms

```{r, include=FALSE}
prop <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(prop) <- c("Tertiary_habitat", "proportion")
for (ele in stats_6210$proportion_6210){
  prop[nrow(prop)+1,] <- c("Dry calcareous grasslands", ele)
  
}
for (ele in stats_6230$proportion_6230){
  prop[nrow(prop)+1,] <- c("Nardus grasslands", ele)
  
}

prop$proportion <- as.numeric(prop$proportion)
```


```{r, echo = FALSE, message=FALSE, warning = FALSE}
ggplot()+
  geom_histogram(data=prop, aes(x = proportion, fill=Tertiary_habitat))+
  facet_wrap(~Tertiary_habitat)+
  scale_fill_manual(values=c("dodgerblue3", "sienna3"))+
  labs(fill = "Tertiary habitat", alpha = 0.9)+
  ylab("Count")+
  ggtitle("Proportion of species in presence/absence data also found in cover for each plot") +
  xlab("Proportion")
```

There are no big differences between the two histograms. The most important thing to notice from this histogram is that the histogram for "Dry calcareous grasslands" has more plots with a very low proportion. In "Dry calcareous grasslands" 1.9% of the plots have proportion below 0.1 while this is only the case for 1.2% of the plots in "Nardus grasslands".
<br>

We suspect that the beta binomial cover update method has most impact on the plots with low proportion. To illustrate this point we plot the posterior predictive p-value for each plot against the proportion of species found in both cover data and presence/absence data for that plot. The posterior predictive p-values come from the posterior predictive checks where the Hill Shannon was used as test statistic. The y-axis is -log(posterior predictive p-value) and the dashed line is equal to -log(0.05) for comparison. Because we have taken the negative logarithm on the posterior predictive p-value the plots above the dashed line are more extreme.

```{r,include=FALSE}
stats_6230$pval1 <- -log(stats_6230$pval)
stats_6210$pval1 <- -log(stats_6210$pval)

```

```{r, include=FALSE}
stat <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(stat) <- c("Tertiary_habitat", "proportion", "pvalue")
for (ele in 1:nrow(stats_6210)){
  stat[nrow(stat)+1,] <- c("Dry calcareous grasslands", stats_6210[ele, 8], stats_6210[ele, 10])
  
}
for (ele in 1:nrow(stats_6230)){
  stat[nrow(stat)+1,] <- c("Nardus grasslands", stats_6230[ele, 8], stats_6230[ele, 10])
}
```

```{r, include = FALSE}
stat$proportion <- as.numeric(stat$proportion)
stat$pvalue <- as.numeric(stat$pvalue)
```



```{r, echo = FALSE, message=FALSE, warning = FALSE}
ggplot() +
  geom_point(data = stat, aes(x= proportion, y=pvalue, col = Tertiary_habitat))+
  geom_smooth(data = stat, aes(x= proportion, y=pvalue), se = FALSE)+
  facet_wrap(~Tertiary_habitat)+
  scale_y_continuous("-log(posterior predictive p-values)")+
  geom_hline(yintercept=-log(0.05), color = "black", linetype="dashed", alpha = 0.5) +
  scale_colour_discrete("Tertiary habitat")+
  scale_x_continuous("Proportion of species in presence/absence data also found in cover")
 
```


The above scatter plots nicely visualize the fact that plots with a low proportion are more sensitive to the beta binomial cover update method. We see that when a plot has a low proportion the observed Hill Shannon diversity is far more likely to be extreme relative to the generated Hill Shannon diversities we get from the beta binomial cover update method, i.e. the model has had a big effect on the plot. Furthermore, we notice that all plots with a posterior predictive p-value less than 0.05 have a proportion less than 0.34. This fits nicely with what we expected and is probably the reason that our method has a slightly bigger effect on "Dry calcareous grasslands" than "Nardus grasslands" as "Dry calcareous grasslands" has a bit more plots where the proportion of species that are found in the presence/absence data and also found in the cover data is very low. 

Additionally, this last discussion is in line with some of the findings we made under the *Comparing diversity measures* section on [Examples](example.html). Here we also saw that the `beta_binomial_cover_update` function does not seem to have a big effect on plots where most species are spotted on both the cover data and the presence/absence data.

<h2>Other validations </h2>
By using the posterior predictive checks we studied if the observed data is a likely outcome of our constructed model. It turned out to be the conclusion in most cases. However, the observed data is only a sample for each plot. This means that the "true" diversity of a plot might be rather different than the information we get from the observed cover and presence/absence data. The more correct diversity estimate could be obtained if we had cover data for the entire plot and not only the small square in the center of the plot. This will, however, be a huge job to collect. The beta binomial cover update method  should in theory give a good estimate of the cover of a species in the entire plot based on the sample from the cover data in the small square and the corresponding presence/absence data of the entire plot. 

An interesting thing to do would be to simulate a dataset (both cover and present/absence) with known diversities for the plots in the dataset. When knowing the true diversity we could have tested how much better our method would be to estimate the diversity rather than only using the cover or presence/absence data. However, this validation of the method has not been possible within the scope of this project and is a topic for further studies in the future. <br>


Building the model we assumed that a species follow the same spatially aggregated distribution within the same tertiary habitat type. This assumption gave reasonable results. However, we have not tested if it would be accurate to assume that the spatially aggregated distribution of species can be created across different tertiary habitats. One could therefore also investigate this subject further. 

<br><br>

<h2> Code implementation </h2>

The code for the implementation of the `ppc` function can be found here: ["Code implementation"](ppc_code.html).

<h2>References </h2>


