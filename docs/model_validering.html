<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Model validation</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/master/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
      </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">DataProject</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Introduction.html">Introduction</a>
</li>
<li>
  <a href="Diversity.html">Diversity</a>
</li>
<li>
  <a href="Model.html">Bayesian model</a>
</li>
<li>
  <a href="Function_usage.html">Functions</a>
</li>
<li>
  <a href="example.html">Examples</a>
</li>
<li>
  <a href="model_validering.html">Model validation</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/nbendsen/DataProject">
    <span class="fa fa-github"></span>
     
    Source code
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Model validation</h1>

</div>


<h2>
Introduction
</h2>
<p>On the Bayesian model page we introduced the beta binomial update method. The method combines observed cover data from the small square in the center of the plot and the presence/absence data from the entire plot. This gave rise to a new updated dataset that could be used to give diversity estimates of the entire plot. This page is intended check the goodness-of-fit of the proposed method against the observed cover data.</p>
<p>When we created the method we assumed that it would describe the data well. Especially, we assumed that the diversity estimates we would get by using the beta binomial update method did not differ too much from the diversity estimates we would achieve by only using the observed cover data. However, this is only an assumption and not necessarily true. We did however assume that the diversity estimates would be a bit higher using the beta binomial update method instead of using the observed data, since we add a small value for species present in the plot, but not observed in the small square of the plot, which the cover data does not. One issue could be that in plots with low diversity, the prior distributions will change the data too much so that our model will estimate the diversity way too high compared to the diversity estimate we would get from the observed cover data of the small square. <br></p>
<p>The intention of this page is to validate whether this assumption is acceptable. It will be done by using the ideas of posterior predictive checks, which are presented below and further described in <span class="citation">[1]</span> and <span class="citation">[2]</span>.</p>
<h2>
Posterior predictive checks
</h2>
<p>In each plot we obtain a posterior distribution for each specie that have a <span class="math inline">\(1\)</span> in the corresponding presence/absence data of the plot. These posterior distributions are estimated using the observed cover data and observed presence/absence data.</p>
<p>For a given species in a plot the posterior distribution is given by</p>
<p><span class="math display">\[
\text{Posterior} \sim Beta(a+y,b+n-y)
\]</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the parameters from the estimated prior distribution for the species, <span class="math inline">\(y\)</span> is the number of hits by the species in the cover data and <span class="math inline">\(n\)</span> is the total number of possible hits for a species in a plot. In the case of the NOVANA dataset, we have <span class="math inline">\(n = 16\)</span>.</p>
<p>In our method we use the mean of the posterior distribution as an estimate of the species cover in the entire plot. Instead of using the mean for the validation we draw one sample from the posterior distribution for each species we know is present in the plot and use this as the estimate of the species cover in the entire the plot. We will refer to this as generated cover data. After the sampling we apply some test statistic of interest on the generated cover data for the plot. This test statistic should capture some of the aspects on the data we interested in. In our case we want the method to be used to estimate diversity, so using some sort of diversity estimator as test statistic is natural. We do this for all plots in the dataset.</p>
<p>We repeat the above process a 1000 times so that we get a distribution of generated test statistics for each plot. Lastly, we compare the test statistic we would get by using the observed cover data with the distributions of generated test statistics from the generated data. Again we do this for each plot. We refer to the test statistic we get from the observed cover data as the observed test statistic.</p>
<p>The idea behind posterior predictive checks is as follow: If the model assumption are appropriate, the generated data will look like the observed data viewed through the chosen test statistic. Especially, if we make a histogram of the generated test statistics the observed test statistic should not be an outlier. If it turns out that the observed test statistic is extreme compared to the generated test statistics it would cause some concern regarding whether the model is appropriate.</p>
<p>The posterior predictive check can be done visually as is done for the first 3 plots in the examples below. In addition to visual inspections of the histograms we can also calculate the tail-area probability, which we call the posterior predictive p-value [3]. If we let <span class="math inline">\(T(y)\)</span> be the observed test statistic and <span class="math inline">\(T(y^{rep})\)</span> be the distribution of generated test statistics, then we calculate the posterior predictive p-value as</p>
<p><span class="math display">\[
\text{posterior predictive p-value} = 2\cdot \min\Big(P(T(y^{rep}) \leq T(y)),P(T(y^{rep}) \geq T(y)) \Big)
\]</span></p>
<p>A small posterior predictive p-value close to zero indicates that the observed test statistic is not very likely relative to the generated test statistics and the posterior predictive check suggests that the model is misspecified with respect to the test statistic. The reason is, that when generating the test statistics and and forming the histogram, we observe a distribution for the generated test statistics. When the posterior predictive p-value is small, the observed test statistic will have a small probability of occurring in the generated distribution for the test statistics, hence will not be likely to occur and thereby is not similar to the generated test statistics. <br> As mentioned earlier we will use diversity estimates as test statistics. We saw on the <a href="example.html">example page</a> that most plots in the beta binomial cover dataset had higher diversity estimates than in the observed dataset. However, that was not the case for all plots. We have defined the posterior predictive p-value as above to take into account when our method returns diversity estimate too high or too low relative to the observed dataset as we see both outcomes as extreme.</p>
<p>The posterior predictive checks are only intended to highlight if our model is likely given the data, i.e. the model fit the observed data well. This, however, does not mean that the model is “better” to estimate diversity than just using cover or presence/absence data. This is a further assumption we need to make based on considerations within the domain of ecology.</p>
<h2>
Model validation for “Nardus grasslands”
</h2>
<p>In the two examples <em>“Comparison”</em> and <em>“Species diversity and nitrogen deposition”</em> on the <a href="example.html">example page</a> we worked with a subset of the NOVANA dataset from the tertiary habitat “Nardus grasslands”. Further details on how this subset was created can be seen here <a href="data_6230.html">data 6230</a>. In this subsection we will apply the ideas of posterior predictive checks to see how the beta binomial update method performs on this subset. <br></p>
<p>As test statistic we will use the Hill Simpson and Hill Shannon diversities that were also used in the example on the <a href="example.html">example page</a>. Especially, we saw in the <em>“Comparison”</em> on the example page how the diversity estimates obtained by the beta binomial cover dataset drifted further apart from the diversity estimates of the observed cover data when applying the the Hill Shannon diversity instead of the Hill Simpson diversity. Therefore, it is of interest to see how the properties of the two different ways to estimate diversity unfold when they are used as test statistics in our posterior predictive checks. Once again we define these as <span class="math display">\[
\text{Hill Simpson = } \frac{1}{\Sigma_{i=1}^S(p_i)^2}
\]</span> <span class="math display">\[
\text{Hill Shannon  = } e^{ - \Sigma_{i = 1}^Sp_i\cdot \ln(p_i)}
\]</span></p>
<p>Further details on the diversity estimates can be found in <a href="Diversity.html">Diversity</a>.</p>
<p>We read in the datasets and remove the first 3 columns, since they do not contain information on species.</p>
<pre class="r"><code>#Here we load the datasets for habitat 6230 in year 2014
cover &lt;- read.csv(&quot;data/cover_data_6230_year2014.csv&quot;)
freq &lt;- read.csv(&quot;data/frekvens_data_6230_year2014.csv&quot;)
#We remove the first 3 columns as they are not species
cover_data &lt;- cover[,4:ncol(cover)]
freq_data &lt;- freq[,4:ncol(freq)]</code></pre>
<p>With the data we make a visual inspection of the first 3 plots where the Hill Shannon diversity is used as a test statistic. A description of the code implementation of the ppc()-function can be found in <a href="ppc_code.html">“Code implementation”</a>.</p>
<pre class="r"><code>ppc(1, freq_data, cover_data)</code></pre>
<p><img src="figure/model_validering.rmd/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ppc(2, freq_data, cover_data)</code></pre>
<p><img src="figure/model_validering.rmd/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ppc(3, freq_data, cover_data)</code></pre>
<p><img src="figure/model_validering.rmd/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>All posterior predictive p-values are big in the above histograms which suggests that the model is appropriate for the first three plots viewed through the Hill Shannon diversity. If we run the posterior predictive check on all plots we can get the proportion of posterior predictive p-values that are less than 0.05. We use a posterior predictive p-value of 0.05 as a threshold for when to say that the observed test statistic is not very likely given the model.</p>
<pre class="r"><code>p_values_hill_shannon &lt;- read.csv(&quot;data/hill_shannon_pval2.csv&quot;)
sum(p_values_hill_shannon &lt; 0.05) / nrow(cover)</code></pre>
<pre><code>[1] 0.07873377</code></pre>
<p>The interpretation is that for around 8% of the plots the posterior predictive check suggests that the beta binomial update method is not appropriate, i.e. the observed Hill Shannon diversity is an outlier compared to the Hill Shannon diversities that the model would generate.</p>
<p>However, it is important to check if these plots are randomly placed or lumped together for either high or low diversities. To do this we plot the mean of all the generated Hill Shannon diversities for each plot against the observed Hill Shannon diversity for the corresponding plot. We color each point in the scatter plot to visualize the posterior predictive p-value of the plot.</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It is worth to notice that it is especially plots with low diversity that our model seems to give diversity estimates that differ a lot from what we would get from the observed cover data. However, this does not necessarily mean that our model is bad. What it does mean is that it is more likely for these plots that the beta binomial update method overestimate the diversity for the entire plot relative to the observed cover data of the small square. Otherwise the beta binomial update method seems to generate data that is aligned with the observed cover data.</p>
<p>Lastly, we will create a similar plot but with the Hill Simpson diversity used as test statistic. On the <em>“Comparison”</em> page on the <a href="example.html">example page</a> we saw that the Hill Simpson diversity was the diversity estimator that gave a diversity estimate that differed the least between the beta binomial updated cover data and the observed cover data.</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Again we see that it is only for plots with low diversity where the observed Hill Simpson diversity seems unlikely given the model. However, this is only a very small proportion.</p>
<pre class="r"><code>sum(p_values_hill_simpson$p_values &lt; 0.05)/nrow(cover)</code></pre>
<pre><code>[1] 0.01136364</code></pre>
<p>This shows that when the Hill Simpson diversity is used as test statistic the model will generate data that looks very much like the observed data. Much more than the case was when we used the Hill Shannon diversity as test statistic.</p>
<p>This matches with what we found in the “Hill Diversity comparison” section on the example page under <em>“Comparison”</em>. There we also saw that the beta binomial update method has less importance viewed through the Hill Simpson diversity than through the Hill Shannon diversity.</p>
<p>For both test statistics the conclusion is that the model mostly generated data that looks like the observed data. This gives confidence in the fact that the model has captured some good aspects of the observed data and gives sensible and thereby useful result to work with. Again it should be emphasized that it does not mean the the model gives better diversity estimates then what could be obtained by only using the observed cover data.</p>
<h2>
Model validation for “dry calcareous grasslands” example
</h2>
<p>We also want to include the posterior predictive checks we get when we apply these to the data from tertiary habitat “dry calcareous grasslands” from year 2009 that we used in our example about “dry calcareous grasslands” on the example page. We make similar plots as we did above. First with the Hill Shannon diversity as test statistic and then with the Hill Simpson diversity as test statistic.</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Again we are interested in the proportion of plots where the observed test statistic is an outlier compared to the distribution of generated test statistics. For the Hill Shannon diversity the proportion is almost 13% if we again use a posterior predictive p-value of 0.05 as a threshold.</p>
<pre class="r"><code>sum(p_values$p_values &lt; 0.05)/nrow(cover_6210)</code></pre>
<pre><code>[1] 0.1278254</code></pre>
<p>And the proportion of plots where the observed Hill Simpson diversity is an outlier relative to the generated Hill Simpson diversities is only around 2%.</p>
<pre class="r"><code>sum(p_values_simpson$p_values &lt; 0.05)/nrow(cover_6210)</code></pre>
<pre><code>[1] 0.020265</code></pre>
<p>In the case of both test statistics we see that a larger proportion of the plots have posterior predictive p-values indicating that the observed test statistic is more unlikely under the model than in “Nardus grasslands” (first example). As an example, in “Nardus grasslands” the proportion of posterior predictive p-values below 0.05 with the Hill Shannon used as test statistic was only around 8%. However, the proportion of plots is still not so big that it causes concerns regarding the whether the assumptions underlying the model are appropriate.</p>
<p>Below we study if there are some differences in the data from “Nardus grasslands” and “dry calcareous grasslands” that could explain why the model perform worse for the “dry calcareous grasslands” in these posterior predictive checks.</p>
<h2>
Comparison of tertiary habitat types
</h2>
<p>First of all, we look at some statistics to compare the plots in “dry calcareous grasslands” (6210 in NOVANA) and the plots in “Nardus grasslands” (6230 in NOVANA). If we compare the two tertiary habitat types we see that the number of species in the habitat type “dry calcareous grasslands” is generally higher than in the habitat type “Nardus grasslands” at plot level. <br> We look at the presence/absence data for the “Nardus grasslands” and “dry calcareous grasslands” respectively. For each dataset we sum the number of present species in each plot and take the mean of all these. In the presence/absence data for “dry calcareous grasslands” the mean number of species in a plot is 32.6 species while it is 26.8 species in the presence/absence data for “Nardus grasslands”. <br></p>
<p>We do the same with the cover data for both datasets respectively, i.e. we sum the number of species in a plot if the species is hit by at least one pin and take the mean over all plots. In the observed cover data for “dry calcareous grasslands” the average observed number of species in a plot is 10.5 while it is 8.3. in the observed cover data for “Nardus grasslands”. <br></p>
<p>What is really interesting is the the proportion of a plot where a species has a <span class="math inline">\(1\)</span> in the presence/absence data and at least one observation in the corresponding cover data. If this proportion is low we suspect these plots to be the ones where the beta binomial update method has the biggest impact. For each plot we take the number of species found in the cover data and divide this by the number of species found in the presence/absence data for that plot. A species is found in the cover data for a plot if it is hit be at least one pin. If we again just take the average we find that the average proportion for each plot in “dry calcareous grasslands” is 0.33 and the average proportion for each plot in “Nardus grasslands” is 0.32. Again there are no big differences in the statistics between the two tertiary habitat types. <br><br> However, we want to study this last statistic a bit further. To visualize the distribution of the proportions from the plots in each of the two tertiary habitat types we make the following histograms</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>There are no big differences between the two histograms. The most important thing to notice from this histogram is that the histogram for “dry calcareous grasslands” has more plots with a very low proportion. In “dry calcareous grasslands” 1.9% of the plots have proportion below 0.1 while this is only the case for 1.2% of the plots in “Nardus grasslands”. <br></p>
<p>We suspect that the beta binomial update method has most impact on these plots with low proportion. To illustrate this point we plot posterior predictive p-value for each plot against the proportion of a plot where species are found in both cover data and presence/absence data. The posterior predictive p-values come from the posterior predictive checks where the Hill Shannon was used as test statistic. The y-axis is -log(posterior predictive p-value) and the dashed line is equal to -log(0.05) for comparison. Because we have taken the negative logarithm on the posterior predictive p-value the plots above the dashed line are more extreme.</p>
<p><img src="figure/model_validering.rmd/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The above scatterplots nicely visualize the fact that plots with a low proportion are more sensitive to the beta binomial update method. We see that when a plot has a low proportion the observed Hill Shannon diversity is far more likely to be different than the generated Hill Shannon diversities we get from the beta binomial update method. Furthermore, we notice that all plots with a posterior predictive p-value less than 0.05 have a proportion less than 0.34. This fits nicely with what we expected and is probably the reason that our method has a slightly bigger effect on “dry calcareous grasslands” than “Nardus grasslands” as “dry calcareous grasslands” has a bit more plots where the proportion of species that are found in the presence/absence data are also found in the cover data is very low.</p>
<h2>
Other validations
</h2>
<p>By using the posterior predictive checks we studied if the observed data is a likely outcome of our constructed model. It turned out to be the conclusion in most cases. However, the observed data is only a sample for each plot. This means that the “true” diversity of a plot might be rather different than the information we get from the observed cover and presence/absence data. The more correct diversity estimate could be obtained if we had cover data for the entire plot and not only the small square in the center of the plot, this will however be a huge job to collect. The beta binomial update method that we have created should in theory help to give a better diversity estimate based on the sample from the cover data in the small square and the corresponding presence/absence data of the entire plot, since we include all species present in the entire plot.</p>
<p>An interesting thing to do would be to simulate a dataset (both cover and present/absence) with known diversities for the plots in the dataset. When knowing the true diversity we could have tested how much better our method would be to estimate the diversity rather than only using the cover or presence/absence data. However, this validation of the method has not been possible within the scope of this project and is a topic for further studies in the future. <br><br></p>
<h2>
Code implementation
</h2>
<p>The code for the implementation of the ppc can be found here: <a href="ppc_code.html">“Code implementation”</a>.</p>
<h2>
References
</h2>
<div id="refs">
<div id="ref-MimnoE3441">
<p>[1] Mimno, D.; Blei, D.M. and Engelhardt, B.E.: Posterior predictive checks to quantify lack-of-fit in admixture models of latent population structure, Proceedings of the National Academy of Sciences, <strong>112</strong> (2015), no. 26, pp. E3441–E3450.</p>
</div>
<div id="ref-Gelman">
<p>[2] Gelman, A.; Meng, X.-L. and Stern, H.: POSTERIOR predictive assessment of model fitness via realized discrepancies, Statistica Sinica, <strong>6</strong> (1996), no. 4, pp. 733–760.</p>
</div>
</div>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 4.0.3 (2020-10-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 19042)

Matrix products: default

locale:
[1] LC_COLLATE=English_United Kingdom.1252 
[2] LC_CTYPE=English_United Kingdom.1252   
[3] LC_MONETARY=English_United Kingdom.1252
[4] LC_NUMERIC=C                           
[5] LC_TIME=English_United Kingdom.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] fitdistrplus_1.1-3 forcats_0.5.0      stringr_1.4.0      dplyr_1.0.2       
 [5] purrr_0.3.4        readr_1.4.0        tidyr_1.1.2        tibble_3.0.4      
 [9] ggplot2_3.3.2      tidyverse_1.3.0    survival_3.2-7     MASS_7.3-53       

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.5       lubridate_1.7.9  lattice_0.20-41  assertthat_0.2.1
 [5] rprojroot_1.3-2  digest_0.6.25    R6_2.4.1         cellranger_1.1.0
 [9] backports_1.1.10 reprex_0.3.0     evaluate_0.14    httr_1.4.2      
[13] pillar_1.4.6     rlang_0.4.8      readxl_1.3.1     rstudioapi_0.11 
[17] blob_1.2.1       Matrix_1.2-18    rmarkdown_2.4    labeling_0.4.2  
[21] splines_4.0.3    munsell_0.5.0    broom_0.7.2      compiler_4.0.3  
[25] httpuv_1.5.5     modelr_0.1.8     xfun_0.18        pkgconfig_2.0.3 
[29] mgcv_1.8-33      htmltools_0.5.0  tidyselect_1.1.0 workflowr_1.6.2 
[33] fansi_0.4.1      crayon_1.3.4     dbplyr_1.4.4     withr_2.3.0     
[37] later_1.1.0.1    grid_4.0.3       nlme_3.1-149     jsonlite_1.7.1  
[41] gtable_0.3.0     lifecycle_0.2.0  DBI_1.1.0        git2r_0.28.0    
[45] magrittr_1.5     scales_1.1.1     cli_2.1.0        stringi_1.5.3   
[49] farver_2.0.3     fs_1.5.0         promises_1.1.1   xml2_1.3.2      
[53] ellipsis_0.3.1   generics_0.0.2   vctrs_0.3.4      tools_4.0.3     
[57] glue_1.4.2       hms_0.5.3        yaml_2.2.1       colorspace_1.4-1
[61] rvest_0.3.6      knitr_1.30       haven_2.3.1     </code></pre>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
