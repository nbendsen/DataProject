---
title: "Model validation"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

---
title: "Model validation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(MASS)
library(survival)
library(tidyverse)
```

<h1>Assumption</h1>

When we presented the new method to obtain better diversity estimates
at the home page, we made an implicit assumption. We assumed that the model 
we had created with the use of an empirical bayes framework describes the data well.
Especially, we assume that the diversity estimates we would get by using our method
do not differ too much from the diversity estimates we would achieve by only using 
the observed cover data.
However, this is only an assumption and not necessarily true. One issue could be that in plots with low diversity, the prior distributions will change the data too much so that our model will estimate the diversity way too high. The intention of this page is to 
validate whether this assumption is acceptable. It will be done by using the ideas of
posterior predictive checks, which are presented below and further described in [1] and [2].

<h1>Posterior predictive checks</h1>
In each plot we have obtained a posterior distribution for each specie that have a $1$ in the corresponding presence/absence data for this plot. These posterior distributions are estimated using the observed data.

For a given species in a plot the posterior distribution is given by 

$$
\text{Posterior} \sim Beta(a+y,b+n-y)
$$
where $a$ and $b$ are the parameters from the estimated prior distribution for the species, $y$ is the number of hits in the cover data and $n$ is the total number of possible hits for a species in a plot. In the case of the NOVANA dataset, we have $n = 16$.


In our method, we use the mean of the posterior distribution as an estimate of the species cover in the entire plot. Now, instead of using the mean we draw one sample from the distribution for each species we know is present in the plot and use this as the cover of the species in the plot. We will refer to this as generated cover data. After the sampling we apply some test statistic of interest on the generated cover data for the plot. This test statistic should capture some of the aspects on the data we interested in. In our case we want the method to be used to estimate diversity, so using some sort of diversity estimator as test statistic is natural. We do this for all plots in the chosen dataset.

We repeat the above  process a 1000 times so that we get a distribution of generated test statistics. Lastly, we compare the test statistic we would get by using the observed cover data for each plot with the distributions of generated test statistics. We refer to the test statistic we get from the observed cover data as the observed test statistic. 


The idea behind posterior predictive checks is as follow: if the model assumption are appropriate, the generated data will look like the observed data viewed through the chosen test statistic. Escpecially, if we make a histogram of the generated test statistics, the observed test statistic should lie within a reasoble range of the center of the histogram and not too far out to any of the sides. If the latter is the case, the observed test statistic is extreme compared to the generated test statistics and this would cause some concern regarding if the model is appropriate.

The posterior predictive check can be done visually as is done for the first 3 plots in the examples below.
In addition to visual inspections of the histograms, we  can also calculate the tail-area probability which we call the posterior predictive p-value [3]. If we let $T(y)$ be the observed test statistic and $T(y^{rep})$ be the generated test statistics, then we calculate the posterior predictive p-value as
$$
\text{posterior predictive p-value} = P(T(y^{rep}) \leq T(y))
$$
Small p-value close to zero indicate that the observed test statistic is not very likely relative to the generated data and the posterior predictive check suggests that the model is misspecified with respect to the test statistic. 

The posterior predictive checks are only intended to highlight if our model is likely given the data. This does not mean that the model is "better" to estimate diversity than just using cover or presence/absence data, only that the model fit the observed data well. When we apply posterior predictive checks, we will be very  interested in the proportion of plots that have a posterior predictive p-value below $0.05$.

In the examples below, we use different diversity estimators as test statistics. We saw on the home page that when applying the beta binomial cover update function, the diversity estimate of a plot will increase; this was case for almost all plots except a few when Simpson's index or Hill Simpson diversity was used. Therefore, we want the posterior predictive p-value to indicate whether the method has increased the diversity estimate too much relative to the observed cover data as this would be an extreme outcome from our model. Of this reason, it is the left-side tail-area we will denote as the posterior predictive p-value. 


<h1> Model validation for first example </h1>
In the first example on the home page we worked with a subset the NOVANA dataset from the tertiary habitat "Surt overdrev" created at [link](data_6230.html). In this subsection we will apply the ideas of posterior predictive checks to see how the method performs on this subset.

As test statistic we will use the Hill Simpson and Hill Shannon diversities that were also used in the example on the home page. Especially, we saw in the example on the home page how the diversity estimate of the observed cover data drifted further apart from the diversity estimate obtained by the beta binomial cover updated dataset when applying the the Hill Shannon diversity instead of the Hill Simpson. Therefore, it is of interest to see how the properties of the two different ways to estimate diversity unfold when they are used as test statistics for our posterior predictive checks. Once again we define these as 

$$
\text{Hill Simpson = } \frac{1}{\Sigma_{i=1}^S(p_i)^2}
$$

$$
\text{Hill Shannon  = } e^{ - \Sigma_{i = 1}^Sp_i\cdot \ln(p_i)}
$$


We read in the datasets and remove the first 3 columns

```{r}
#Here we load the datasets for habitat 6230 in year 2014

cover <- read.csv("data/cover_data_6230_year2014.csv")

freq <- read.csv("data/frekvens_data_6230_year2014.csv")

#We remove the first 3 columns, as they are not species

cover_data <- cover[,4:ncol(cover)]

freq_data <- freq[,4:ncol(freq)]

```



With the loaded data we make a visual inspection of the first 3 plots where the Hill Shannon diversity is used as a test statistic

```{r, include= FALSE}
#We load the function for used for the validation
source("code/model_val.R")
source("code/function.R")
```

```{r, warning=FALSE}
ppc(1, freq_data, cover_data)
```

```{r, warning=FALSE}
ppc(2, freq_data, cover_data)
```

```{r, warning=FALSE}
ppc(3, freq_data, cover_data)
```

All posterior predictive p-values are big in the above histograms which suggests that the model is appropriate for the first three plots viewed through the Hill Shannon diversity. If we run the posterior predictive check on all plots we can get the proportion of posterior predictive p-values that are less than 0.05

```{r}
p_values_hill_shannon <- read.csv("data/hill_shannon_pval1.csv")
sum(p_values_hill_shannon < 0.05) / nrow(cover)
```


This proportion is a bit higher than expected. It says that for around 16% of the plots the Hill Shannon we would generate from our model, the observed Hill Shannon diversity would not be likely. However, it is important to check if the plots where our model have performed badly is randomly placed or lumped together for either high or low diversities. To do this we plot the mean of the generated Hill Shannon diversity for each plot against the Hill Shannon diversity of the observed cover data. We color each point in the scatter plot to indicate if the corresponding plot has a posterior predictive p-value below 0.05 or above.

```{r}
generated_hill_shannon <- read.csv("data/hill_shannon.csv")[,1:1000]
observed_hill_shannon <- hill_shannon(cover_data)
colnames(p_values_hill_shannon) <- "p_values"
p_values_hill_shannon$mean <- rowMeans(generated_hill_shannon)
p_values_hill_shannon$observed <- observed_hill_shannon
```

```{r, echo = FALSE}
ggplot(data = p_values_hill_shannon, mapping = aes(x = observed, y = mean )) +
  geom_point(mapping = aes(col = ifelse(p_values > 0.05,'Greater','Less')))+
  ylab("Mean of generated Hill Shannon diversity")+
  xlab("Hill Shannon diversity of observed cover data") +
  guides(color=guide_legend("Posterior predictive p-value less than or greater than 0.05")) +
  ggtitle("Mean of generated Hill Shannon diversity vs observed Hill Shannon diversity") +
  geom_abline()
```


It is worth to notice that it is specially plots with low to medium level of diversity that our model seems to give diversity estimates that differ a lot from what we would get from the observed cover data. This, however, is probably not that surprising as these are the plots where the prior distributions of the observed species matter the most. This does not necessarily mean that our model is bad. A closer look at these plots with an ecology expertise would be necessary to evaluate if the result from the model makes sense. Otherwise the beta binomial cover update method seems to generate data that is aligned with the observed data.



Lastly, we will create a similar plot  with the Hill Simpson diversity. In the example on the home page we saw that the Hill Simpson diversity was the diversity estimator that gave a diveristy estimate that differed the least between the beta binomial updated cover data and the observed cover data. 

```{r, include=FALSE, eval=FALSE }
m <- 1000
p <- c()
for ( n in 1:nrow(cover)){
  upper <- sum(generated_hill_simpson[n,1:m]>=observed_hill_simpson[n])/m
  p <- c(p, upper)
  
}
```



```{r}
generated_hill_simpson <- read.csv("data/hill_simpson.csv")[,1:1000]
p_values_hill_simpson <- read.csv("data/hill_simpson_pval1.csv")
observed_hill_simpson <- hill_simpson(cover_data)
colnames(p_values_hill_simpson) <- "p_values"
p_values_hill_simpson$mean <- rowMeans(generated_hill_simpson)
p_values_hill_simpson$observed <- observed_hill_simpson
```

```{r, echo = FALSE}
ggplot(data = p_values_hill_simpson, mapping = aes(x = observed, y = mean )) +
  geom_point(mapping = aes(col = ifelse(p_values > 0.05,'Greater','Less')))+
  ylab("Mean of generated Hill Simpson diversity")+
  xlab("Hill Simpson diversity of observed cover data") +
  guides(color=guide_legend("Posterior predictive p-value less than or greater than 0.05")) +
  ggtitle("Mean of generated Hill Simpson diversity vs observed Hill Simpson diversity") +
  geom_abline()
```


Again we see that it is only for plots with little diversity where the observed Hill Simpson diversity seems unlikely given the model. However, this is only a very small proportion 

```{r}
sum(p_values_hill_simpson$p_values < 0.05)/nrow(cover)
```

This shows that when the Hill Simpson diversity is used as test statistic the model will generate data that very much looks like the observed data. This does not come to a big surprise given that the Hill Simpson emphasize common species and that our method probably has a bigger effect on rare species than common. This matches with what we found out, when we compared the different hill diversities, with both the beta binomial updated data, and just the cover data.

For both test statistics the conclusion is that the model generate data that looks like the observed data. This gives confidence in the fact that the model has captured some good aspects of the data and gives sensible and thereby useful result to work with. Again it should be emphasized that it does not mean the the model gives better diversity estimates then what could be obtained by only using the observed cover data.



<h1>Model validation for second example </h1>

We also want to briefly show the posterior predictive checks we get when we apply these to the data from tertiary habitat "Kalkoverdrev" from year 2009 that we used in our second example on the home page. We make a similar plot as we did above with the Hill Shannon diversity as test statistic

```{r, include=FALSE, eval=FALSE}
m <- 1000
pva <- c()
for (n in 1:nrow(cover_6210)){
  lower <- sum(generated_hill_shannon6210[n,1:m]<=observed_hill_shannon6210[n])/m
  pva <- c(pva, lower)
}
```



```{r}
cover_6210 <- read.csv("data/cover_data_6210_year2009.csv")
generated_hill_shannon6210 <- read.csv("data/hill_shannon_6210.csv")[,1:1000]
observed_hill_shannon6210 <- hill_shannon(cover_6210, 3)
p_values <- read.csv("data/6210_hill_shannon_pvalues.csv")
colnames(p_values) <- "p_values"
p_values$mean <- rowMeans(generated_hill_shannon6210)
p_values$observed <- observed_hill_shannon6210

```

```{r, echo = FALSE}
ggplot(data = p_values, mapping = aes(x = observed, y = mean )) +
  geom_point(mapping = aes(col = ifelse(p_values > 0.05,'Greater','Less')))+
  ylab("Mean of generated Hill Shannon diversity")+
  xlab("Hill Shannon diversity of observed cover data") +
  guides(color=guide_legend("Posterior predictive p-value less than or greater than 0.05")) +
  ggtitle("Mean of generated Hill Shannon diversity vs observed Hill Shannon diversity") +
  geom_abline()
```


```{r}
sum(p_values$p_values < 0.05)/nrow(cover_6210)
```

We see that a higher proportion of the plots have generated Hill Shannon diversities where the corresponding observed Hill Shannon diversity is an unlikely outcome from the model. 


```{r, include= FALSE}
freq_6210 <- read.csv("data/frekvens_data_6210_year2009.csv")
```


```{r, include = FALSE}
stats_6230 <- cover[1:3]
stats_6210 <- cover_6210[1:3]

stats_6230$freq_sum_6230 <- rowSums(freq[4:ncol(freq)])
stats_6210$freq_sum_6210 <- rowSums(freq_6210[4:ncol(freq_6210)])

mean_freq_6230 <- mean(stats_6230$freq_sum_6230)
mean_freq_6210 <- mean(stats_6210$freq_sum_6210)

stats_6230$cover_sum_6230 <- rowSums(cover[4:ncol(cover)] >  0)
stats_6210$cover_sum_6210 <- rowSums(cover_6210[4:ncol(cover_6210)] > 0 )


stats_6230$cover_mean_over_0_6230 <- rowMeans(replace(cover[4:ncol(cover)], cover[4:ncol(cover)] == 0, NA), na.rm = TRUE)
stats_6210$cover_mean_over_0_6210<- rowMeans(replace(cover_6210[4:ncol(cover_6210)], cover_6210[4:ncol(cover_6210)] == 0, NA), na.rm = TRUE)

mean_cover_mean_over_0_6230 <- mean(stats_6230$cover_mean_over_0_6230)
mean_cover_mean_over_0_6210 <- mean(stats_6210$cover_mean_over_0_6210)

mean_cover_6230 <- mean(stats_6230$cover_sum_6230)
mean_cover_6210 <- mean(stats_6210$cover_sum_6210)

stats_6230$difference_6230 <- stats_6230$freq_sum_6230 - stats_6230$cover_sum_6230
stats_6210$difference_6210 <- stats_6210$freq_sum_6210 - stats_6210$cover_sum_6210

stats_6230$proportion_6230 <- stats_6230$cover_sum_6230 / stats_6230$freq_sum_6230 
stats_6210$proportion_6210 <- stats_6210$cover_sum_6210 / stats_6210$freq_sum_6210

mean_proportion_6230 <- mean(stats_6230$proportion_6230)
mean_proportion_6210 <- mean(stats_6210$proportion_6210)

stats_6210$pval <- p_values$p_values
stats_6230$pval <- p_values_hill_shannon$p_values
```


```{r, include = FALSE}
mean(p_values_hill_shannon$observed[p_values_hill_shannon$p_values < 0.05])
mean(p_values$observed[p_values$p_values < 0.05])
```

```{r, include = FALSE}
mean(p_values_hill_shannon$observed)
mean(p_values$observed)
```


If we compare the two habitat types, we see that the number of specie in habitat type 6210 is generally higher than in habitat type 6230. This is true both in specie recorded in presence/absence, where on average 32,6 specie is found in 6210 compared to 26,8 on average in 6230. The same can be seen in the cover data, where on average 10,5 specie is found in 6210 vs 8,3 in 6230. However, the proportion of specie that is found in cover data vs presence/absence data is about the same for the two habitat types at 0.33 vs 0.32. If we look at the average cover for the specie found in cover, are the specie in habitat type 6230 generally more abundance with an average of 5,3 vs 4,9 in 6210. These statistics are all very similar for the two habitat types. The biggest difference is when we compare the average Hill shannon diversity for the two habitat types, where 6210 is 7,5 vs 6230 only at 5,9.



If we compare the distribution of the proportion of specie found in presence/absence data that is also found in cover data, we get about the same distribution for both habitat types.


```{r, echo = FALSE, warnings= FALSE}
ggplot() +
  geom_histogram(data = stats_6230, aes(x = proportion_6230 ), col = "blue", alpha = 0.7) + 
  geom_histogram(data = stats_6210, aes(x= proportion_6210), col = "red", alpha = 0.5)
  
```

Since we suspect that our method changes most in the plot where a small proportion of the specie in the presence/absence data are found in the cover data, will we plot the proportion against the p-value.

```{r, echo = FALSE, warning= FALSE}
ggplot() + 
  geom_point(data = stats_6230, aes(x = proportion_6230, y = pval ), col = "blue", alpha = 0.7 ) +
  geom_point(data = stats_6210, aes(x = proportion_6210, y = pval ), col = "red", alpha = 0.7 ) +
  geom_smooth(data = stats_6230, aes(x = proportion_6230, y = pval ), col = "blue") + 
  geom_smooth(data = stats_6210, aes(x = proportion_6210, y = pval ), col = "red") + 
  geom_hline(yintercept=0.05, color = "black") + 
  geom_vline(xintercept=0.34, linetype="dashed", color = "black") +
  xlab("Propotion of specie in presence/absence also found in cover ") + 
  ylab("p-value") + 
  ggtitle("Propotion plotted against p-value")
```

This plot clearly shows that the plots with a low proportion are far more likely to be significant different. We have also plotted the proportion of 0.34, since all plots with a p-value less than 0.05, has a proportion less than 0.34.





<h1>Other validations </h1>
By using the posterior predictive checks we checked if the observed data is a likely outcome of our constructed model. It turned out to be the case. However, the observed data is only a sample for each plot. This means that the "true" diversity of a plot might be rather different than the information we get from the observed cover and presence/absence data. The more correct diversity estimate could be obtained if we had cover data for the entire plot and not only the small square in the center of the plot. The new method that we have created should in theory help to give a better diversity estimate based on the sample from the cover data in the small square and the corresponding presence/absence data of the entire plot.

An interesting thing to do would be to simulate a dataset with known diversities for the plots in the dataset. When knowing the true diversity we could have tested how much better our method would be to estimate the diversity rather than only using the cover or presence/absence data. However, this validation of the method has not been possible within the scope of this project and is a topic for further studies in the future.


<br><br>


<h1>Code implementation </h1>
```{r, eval=FALSE}
library(fitdistrplus)
```

```{r, eval = FALSE}
#We read the cover data and the presence/absense data without the first 4 columns, as they do not cotains information on species

cover_data <- cover[,4:ncol(cover)]
freq_data <- freq[,4:ncol(freq)]
  
#We make a dataframa for the parameters of the prior distribution for each plot, so it is possible to save the parameteres, and not calculate them when making the posteriror for each plot. Each row will contain the number/name of the specie and its corresponding parameters for the prior 

beta_fit <- data.frame(matrix(ncol = 3, nrow = 0))

# We name the columns in the 
colnames(beta_fit) <- c("species","a", "b")
```



```{r, eval = FALSE}
#Here we calculate the parameters for the priror distributions of each specie:
for (specie in colnames(cover_data)) {
  #First we normalise. Since there is in total used 16 pins for each plot, we will devide the entries in the cover data by 16
  beta_data <- cover_data[,specie]/16
  
  #Now we remove the plots where the specie is not present. This can be done by using the information from the presence, absense data. If it        contains a 1, then the specie is present in the plot, if 0 it is absent. 
  beta_data <- beta_data[freq_data[[specie]] == 1]
    
  #If the specie is not present in any of the plots, we do not have information to make a prior distribution for it, and will just give it parameters a=0 and b=0 as seen in the else clause.
  if (length(unique(beta_data)) > 1) {
    #We use the method of moments to fit the prior beta distribution
    beta_data_fitted <- fitdist(beta_data, "beta", method = "mme")
    
    #The parameters are added to the dataframe
    beta_fit[nrow(beta_fit) + 1,] <- c(specie, beta_data_fitted$estimate[1], beta_data_fitted$estimate[2])
      
  }
  else {
    beta_fit[nrow(beta_fit) + 1,] <- c(specie, 0,0)
    
  }
}

```

```{r, eval = FALSE}
#n is the row number of the plot we are working with
n <- 1

#We define which species are in present in the plot
species_spotted_in_frekvens <- colnames(freq_data[c(freq_data[n,]  == 1)])

#We define which species are present in the present/absent dataset but are not seen in the cover dataset
not_in_cover <- setdiff(species_spotted_in_frekvens,colnames(cover_data))

#We remove the species, that at present in the plots in the present/absent data, but are not observed in any plots in the cover data
species_spotted_in_frekvens <- setdiff(species_spotted_in_frekvens, not_in_cover)

# we remove the columns, that are not representing species
observed <- cover_data[n,c(species_spotted_in_frekvens)]

tmp <- observed[observed > 0]
T_static <- -sum(tmp/sum(observed) * log((tmp/sum(observed))))

#We make a dataframe to save the parameters of the posterior for each spotted specie in the plot
new_beta <- data.frame(matrix(ncol = 3, nrow = 0))
  
colnames(new_beta) <- c("species","a", "b") 

for (species_spotted in species_spotted_in_frekvens ) {
      
  #We define the parameters for the posterior 
      alpha_post <- as.numeric(beta_fit[beta_fit$species == species_spotted,]$a) + cover_data[[species_spotted]][n] 
    beta_post <-  as.numeric(beta_fit[beta_fit$species == species_spotted,]$b) + 16 - cover_data[[species_spotted]][n]
    
      #If the parameters are to small, we change them to 0, since R has a hard time working with them
      alpha_post <- ifelse(alpha_post < 1e-10, 0, alpha_post)
      beta_post <- ifelse(beta_post < 1e-10,0, beta_post)
      
      #We add the parameters to the dataframe
      new_beta[nrow(new_beta) + 1,] <- c(species_spotted, alpha_post, beta_post)}

#We make a vector, to save the shannon indexes produces in each iteration
shannon <- c()

for (i in 1:1000){
  
  #Vector for saving the random generated values from the posterior of each specie
  values <- c()
  for (ele in species_spotted_in_frekvens){
    #These are the parameters, for the posterior of the specie
    a <- as.numeric(new_beta[new_beta$species == ele,]$a)
    b <- as.numeric(new_beta[new_beta$species ==ele,]$b)
    
    # We draw a random number from a beta distribution with the parameters for that specie and add it to the vector
    values <- c(values, rbeta(1,a,b))
    
    
  }
  #We remove the values that are to small
  tmp <- values[ values > 0.00001]
  total <- sum(tmp)
  
  #We calculate the shannon index
  shannon <- c(shannon,-sum(tmp/total * log((tmp/total)))) 
  
}

min_val <- min(T_static, min(shannon)) - 0.1
max_val <- max(T_static, max(shannon)) + 0.1

n <- length(shannon)
pvalue <- 2*min(sum(shannon>= T_static)/n, sum(shannon<= T_static)/n)

#This is the code that produces the histogram over the generated data
hist(shannon, xlim = c(min_val, max_val), main = sprintf("Histogram of simulated shannon indexes for plot %d", n), xlab = "Shannon indexes")
legend("topright", legend = "Red line is observed shannon index")
abline(v = T_static, col = "red" )
```





<h1>References </h1>
<ul>
<li>[1] https://www.pnas.org/content/112/26/E3441#ref-19 </li>
<li>[2] POSTERIOR PREDICTIVE ASSESSMENT OF MODEL FITNESS VIA REALIZED DISCREPANCIES by Andrew Gelman, Xiao-Li Meng and Hal Stern </li>
</ul>
<br><br><br><br>


